{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "30642df3-d77f-4ff5-83d7-81bb5d2b8787",
   "metadata": {},
   "outputs": [],
   "source": [
    "# The MagPy jupyter Notebook manual with examples and usage\n",
    "import sys\n",
    "sys.path.insert(1,'/home/leon/Software/magpy/') # should be magpy2\n",
    "from magpy.stream import *\n",
    "from magpy.core import plot as mp\n",
    "matplotlib.use('TkAgg')\n",
    "%matplotlib inline"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8989d247-1439-47ed-a835-58cc9ee06cab",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Chapter 1: Installation and requirements - is skipped here"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "71aaeb2c-39c9-47a1-aff2-3f1befaee658",
   "metadata": {},
   "outputs": [],
   "source": [
    "# ---------------------------------------\n",
    "# Chapter 2: A quick guide to MagPy\n",
    "# ---------------------------------------"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b5eab933-2825-4f60-ba5b-6d9960eba9fb",
   "metadata": {},
   "outputs": [],
   "source": [
    "# 2.1 Getting started with the python package\n",
    "# ----------------------\n",
    "\n",
    "from magpy.stream import *\n",
    "\n",
    "#Now we can use all methods of this module like reading files (see section 3) and working with the data. \n",
    "\n",
    "data = read(example1)\n",
    "\n",
    "#MagPy has many more modules which will be described in detail below. If we want to plot the data we need the plotting\n",
    "#module, which provides the method timeseries plot `tsplot`. Further information is found in section 4.\n",
    "\n",
    "from magpy.core import plot as mp\n",
    "\n",
    "#This is a \"safe\" import method as we do not shadow any other method. Using the module reference *mp* we can now use any\n",
    "#method containd in the plotting module\n",
    "\n",
    "mp.tsplot(data)\n",
    "\n",
    "\n",
    "# 2.2 MagPy's internal data structure\n",
    "# ----------------------\n",
    "\n",
    "#You can view the array part as follows\n",
    "\n",
    "print(data.ndarray)\n",
    "\n",
    "#The header information is printed by\n",
    "\n",
    "print(data.header)\n",
    "\n",
    "\n",
    "# 2.3 Getting help on methods and modules\n",
    "# ----------------------\n",
    "\n",
    "#Information on individual methods and options can be obtained as follows:\n",
    "\n",
    "help(read)\n",
    "\n",
    "#For specific methods related to e.g. a stream object \"data\":\n",
    "\n",
    "data = read(example1)\n",
    "help(data.fit)\n",
    "\n",
    "#Note that this requires the existence of a \"data\" object, which is obtained e.g. by data = read(...). The help text \n",
    "#can also be shown by directly calling the *DataStream* object method using:\n",
    "\n",
    "help(DataStream().fit)\n",
    "\n",
    "#Another example is shown here:\n",
    "\n",
    "help(mp.tsplot)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1eb394f3-a6d5-42a6-b05e-785be1934e8c",
   "metadata": {},
   "outputs": [],
   "source": [
    "# ---------------------------------------\n",
    "# 3. Reading and writing data\n",
    "# ---------------------------------------"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6c813925-1a22-4e50-8838-662916de5b0d",
   "metadata": {},
   "outputs": [],
   "source": [
    "#MagPy supports many different data formats and thus also any possible conversions between them. \n",
    "#To get a full list including read/write support, use:\n",
    "\n",
    "from magpy.stream import read, SUPPORTED_FORMATS\n",
    "print(SUPPORTED_FORMATS)\n",
    "\n",
    "# 3.1 Reading data\n",
    "# ----------------------\n",
    "\n",
    "#In order to read data from any local or remote data source the `read` method is used. This method is imported as \n",
    "#follows:\n",
    "\n",
    "from magpy.stream import read\n",
    "\n",
    "#Then you can access data sources using  data = read(datasource)\n",
    "#Reading data from a remote webservice:\n",
    "\n",
    "data = read('https://cobs.zamg.ac.at/gsa/webservice/query.php?id=WIC')\n",
    "\n",
    "#The most effective way of selecting a specific time range is by already defining it when importing the data:\n",
    "\n",
    "data = read(example4, starttime=\"2024-05-10\", endtime=\"2024-05-11\")\n",
    "\n",
    "#You will find several example files provided with together with the MagPy package. In order to use the example files \n",
    "#it is recommended to import all methods of the `stream` package as follows:\n",
    "\n",
    "from magpy.stream import *\n",
    "\n",
    "#Then you can access all example files by just reading them as follows:\n",
    "\n",
    "data = read(example1)\n",
    "\n",
    "#- `example1`: [IAGA] ZIP (IAGA2002, zip compressed) file with 1 second HEZ data\n",
    "#- `example2`: [MagPy] Archive (CDF) file with 1 sec F data\n",
    "#- `example3`: [MagPy] Basevalue (TXT) ascii file with DI and baseline data\n",
    "#- `example4`: [INTERMAGNET] ImagCDF (CDF) file with four days of 1 second data\n",
    "#- `example5`: [MagPy] Archive (CDF) raw data file with xyz and supporting data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5dc91217-bffe-4cd1-be63-63fb26516401",
   "metadata": {},
   "outputs": [],
   "source": [
    "# 3.2 Writing\n",
    "# ----------------------\n",
    "\n",
    "#Let's create a quick example by loading some example data set and store it a CSV file. \n",
    "\n",
    "data = read(example1)\n",
    "data.write('/tmp/', format_type='CSV', mode='replace', coverage='hour', filenamebegins='MYTEST_',\n",
    "                           filenameends='.csv', dateformat='%Y%m%d%H', subdirectory='Yj', keys=['x'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e66ac1ee-847a-424b-9e52-884d16529f94",
   "metadata": {},
   "outputs": [],
   "source": [
    "# 3.3 Specific commands and options for read and write\n",
    "# ----------------------\n",
    "\n",
    "#In order to get a full list of i.e. supported `write` types use the following example. Change 'w' \n",
    "#by 'r' to get `read` types. Please checkout the full readme file for all specfic options.\n",
    "\n",
    "from magpy.stream import SUPPORTED_FORMATS\n",
    "print([fo for fo in SUPPORTED_FORMATS if 'w' in SUPPORTED_FORMATS.get(fo)[0]])\n",
    "\n",
    "# 3.3.1 Baseline information in IBFV files (BLV)\n",
    "\n",
    "#How to access and plot such  basevalues and the adopted functions is shown here\n",
    "\n",
    "basevalues = read(example7)\n",
    "func = basevalues.header.get('DataFunctionObject')\n",
    "mp.tsplot(basevalues, ['dx','dy','dz'], symbols=[['o','o','o']], functions=[[func,func,func]])\n",
    "\n",
    "#If you are mainly interested in the adopted baseline data you can use the read *mode* 'adopted'.\n",
    "\n",
    "adoptedbase = read(example7, mode='adopted')\n",
    "mp.tsplot(adoptedbase, ['dx','dy','dz'])\n",
    "\n",
    "#If you want to plot data and original adopted basevalues use\n",
    "\n",
    "mp.tsplot([basevalues,adoptedbase], [['dx','dy','dz']], symbols=[['o','o','o'],['-','-','-']])\n",
    "\n",
    "\n",
    "# 3.3.9 Reading data from the INTERMAGNET Webservice\n",
    "\n",
    "#An important webservice is the [INTERMAGNET webservice](https://imag-data.bgs.ac.uk/GIN/) hosted at the British \n",
    "#Geological Survey (BGS). Below you will find a  typical example of an access using several available options. \n",
    "\n",
    "data = read('https://imag-data-staging.bgs.ac.uk/GIN_V1/GINServices?request=GetData&observatoryIagaCode=WIC&dataStartDate=2021-03-10T00:00:00Z&dataEndDate=2021-03-11T23:59:59Z&Format=iaga2002&elements=&publicationState=adj-or-rep&samplesPerDay=minute')\n",
    "\n",
    "# 3.3.10 Reading DST data\n",
    "\n",
    "#Disturbed storm time indices are provided by [Kyoto](https://wdc.kugi.kyoto-u.ac.jp/dst_realtime/index.html) in a \n",
    "#world data center (WDC) related format. This data can be accessed as follows.\n",
    "\n",
    "data = read(\"https://wdc.kugi.kyoto-u.ac.jp/dst_realtime/202411/dst2411.for.request\")\n",
    "\n",
    "# 3.3.11 The Conrad Observatory webservice\n",
    "\n",
    "#The [Conrad Observatory](https://cobs.geosphere.at) provides an easy-to-use webservice using standardized options. \n",
    "\n",
    "data = read(\"https://cobs.zamg.ac.at/gsa/webservice/query.php?id=WIC\")\n",
    "\n",
    "# 3.3.12 The USGS webservice\n",
    "\n",
    "#The [USGS webservice](https://www.usgs.gov/tools/web-service-geomagnetism-data) allows you accessing realtime data\n",
    "\n",
    "data = read(\"https://geomag.usgs.gov/ws/data/?id=BOU\")\n",
    "\n",
    "# 3.3.13 Getting Index data from the GFZ Potsdam\n",
    "\n",
    "data=read('https://kp.gfz-potsdam.de/app/json/?start=2024-11-01T00:00:00Z&end=2024-11-02T23:59:59Z&index=Kp')\n",
    "\n",
    "# 3.3.14 Accessing the WDC FTP Server\n",
    "\n",
    "data = read('ftp://ftp.nmh.ac.uk/wdc/obsdata/hourval/single_year/2011/fur2011.wdc')\n",
    "\n",
    "# 3.3.15 NEIC data\n",
    "\n",
    "quake = read('https://earthquake.usgs.gov/earthquakes/feed/v1.0/summary/4.5_week.csv')\n",
    "\n",
    "# 3.3.16 NOAA data: ACE, DSCOVR and GOES\n",
    "\n",
    "ace = read(\"https://sohoftp.nascom.nasa.gov/sdb/goes/ace/daily/20221122_ace_swepam_1m.txt\")\n",
    "\n",
    "dscovr_plasma = read(\"http://services.swpc.noaa.gov/products/solar-wind/plasma-3-day.json\")\n",
    "dscovr_mag = read(\"http://services.swpc.noaa.gov/products/solar-wind/mag-3-day.json\")\n",
    "\n",
    "xray = read(\"https://services.swpc.noaa.gov/json/goes/primary/xrays-6-hour.json\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "dd00d34b-fa35-4116-8f35-0644291b3f66",
   "metadata": {},
   "outputs": [],
   "source": [
    "# ---------------------------------------\n",
    "# 4. Figures\n",
    "# ---------------------------------------"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f29e1073-7584-425d-be7b-353de618aa2b",
   "metadata": {},
   "outputs": [],
   "source": [
    "# 4.1 A quick timeseries plot\n",
    "# ----------------------\n",
    "\n",
    "from magpy.stream import example1, read\n",
    "import magpy.core.plot as mp\n",
    "\n",
    "vario = read(example1)\n",
    "mp.tsplot(vario)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f54d07ed-6d09-4054-b3b5-66a6957537f0",
   "metadata": {},
   "outputs": [],
   "source": [
    "# 4.2 Timeseries: using some plot options and saving\n",
    "# ----------------------\n",
    "\n",
    "vario = read(example1)\n",
    "fig, ax = mp.tsplot(vario,keys=['x','y','z'], title=\"Variometer data\", grid=True, width=10, height=2)\n",
    "\n",
    "#You can save this plot to a file using:\n",
    "\n",
    "fig.savefig(\"/tmp/pl_421.png\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c47f4954-696a-4a7e-90f9-6779d4951a52",
   "metadata": {},
   "outputs": [],
   "source": [
    "# 4.3 Timeseries data from multiple streams\n",
    "# ----------------------\n",
    "\n",
    "scalar = read(example2)\n",
    "fig,ax = mp.tsplot([vario,scalar],keys=[['x','y','z'],['f']], title=\"Variometer data\", grid=True, width=10, \n",
    "                    height=2, yranges=[[[21000,21100],[-50,50],[43800,43850]],[[48600,48650]]], ylabelposition=-0.1, \n",
    "                    dateformatter=\"%Y-%m-%d %H\", legend=True, alpha=0.5,\n",
    "                    fill=[[[],[{\"boundary\":0,\"fillcolor\":\"red\"},{\"boundary\":0,\"fillcolor\":\"blue\",\"fillrange\":\"smaller\"}],[]],[[]]])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9092808a-89f4-48e4-bace-9c81c203861f",
   "metadata": {},
   "outputs": [],
   "source": [
    "# 4.4 Symbols and colors\n",
    "# ----------------------\n",
    "\n",
    "basevalue = read(example3)\n",
    "fig,ax = mp.tsplot([basevalue],[['dx','dy','dz']], symbols=[['.','-.x','--o']], \n",
    "                    colors=[[[0.2, 0.2, 0.2],'r','blue']], padding=[[1,0.005,0.5]], height=2)\n",
    "\n",
    "#In a next example we will plot the H components of three observatories. For this we load data from the USGS webservice.\n",
    "\n",
    "data1 = read(\"https://geomag.usgs.gov/ws/data/?id=BOU\")\n",
    "data2 = read(\"https://geomag.usgs.gov/ws/data/?id=NEW\")\n",
    "data3 = read(\"https://geomag.usgs.gov/ws/data/?id=SIT\")\n",
    "fig,ax = mp.tsplot([data1,data2,data3],[['x'],['x'],['x']],colors=[['g'],['r'],['b']],legend=True,height=2)\n",
    "\n",
    "#The obtained plot show the data on similar timescales below each other. ![4.2.4](./magpy/doc/pl_424.png \"Geomag\")\n",
    "#If you want to plot them in a single diagram then just define a single key value.\n",
    "\n",
    "fig,ax = mp.tsplot([data1,data2,data3], [['x']], colors=[['g'],['r'],['b']], legend={\"legendtext\":('BOU', 'SIT', 'NEW')}, height=2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e88d5dbf-4cf5-455d-b261-af2dcab23bd1",
   "metadata": {},
   "outputs": [],
   "source": [
    "# 4.5 Patches, annotations and functions in tsplot\n",
    "# ----------------------\n",
    "\n",
    "variometer = read(example4)\n",
    "patch = {\"ssc\" : {\"start\":datetime(2024,5,10,17,6),\"end\":datetime(2024,5,10,17,8),\"components\":\"x\",\"color\":\"red\",\"alpha\":0.2},\n",
    "                \"initial\": {\"start\":datetime(2024,5,10,17,8),\"end\":datetime(2024,5,10,19,10),\"components\":\"x\",\"color\":\"yellow\",\"alpha\":0.2},\n",
    "                \"main\": {\"start\":datetime(2024,5,10,19,10),\"end\":datetime(2024,5,11,2,0),\"components\":\"x\",\"color\":\"orange\",\"alpha\":0.2},\n",
    "                \"recovery\": {\"start\":datetime(2024,5,11,2,0),\"end\":datetime(2024,5,12,11),\"components\":\"x\",\"color\":\"green\",\"alpha\":0.2}}\n",
    "fig,ax = mp.tsplot([variometer], keys=[['x']], patch=patch, annotate=True, height=2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "36aee770-2e1c-4ec9-9d4e-b12be8ab1dd5",
   "metadata": {},
   "outputs": [],
   "source": [
    "# ---------------------------------------\n",
    "# Chapter 5: Timeseries\n",
    "# ---------------------------------------"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "87531f49-9775-494c-83ad-7784df60d7c7",
   "metadata": {},
   "outputs": [],
   "source": [
    "# 5.1 Get some basic data characteristics and commonly used manipulations\n",
    "# ----------------------\n",
    "\n",
    "data = read(example5)\n",
    "\n",
    "#### 5.1.1 Basic data characteristics\n",
    "\n",
    "# Lets firstly check what data is actually contained in the data stream.\n",
    "# If you want to now which column keys are used in the current data set use\n",
    "\n",
    "print (data.variables())\n",
    "\n",
    "# Some basic column information, particluarly column name and units as asigned to each column key can be obtained by\n",
    "\n",
    "print (data.get_key_name('x'))\n",
    "print (data.get_key_unit('x'))\n",
    "\n",
    "#The following command will give you a an overview about used keys and their asigned column names and units\n",
    "\n",
    "data._print_key_headers()\n",
    "\n",
    "#The amount of data in each column needs to be identical. You can check the length of all columns using\n",
    "\n",
    "print(data.length())\n",
    "\n",
    "#As all columns require the either the same length or zero length, you simply check the length of the time column\n",
    "#for the total amount of individual timesteps. This will be returned by the classic len command\n",
    "\n",
    "print(len(data))\n",
    "\n",
    "\n",
    "#### 5.1.2 Modifying data columns\n",
    "\n",
    "#The following methods allow you modifying individual data columns, move them to other keys or add some new information\n",
    "#into your data set. Lets deal with another example for the following commands and extract data from key 'f' into a simple \n",
    "#numpy array:\n",
    "\n",
    "fdata = read(example2)\n",
    "fcolumn = fdata._get_column('f')\n",
    "print (len(fcolumn))\n",
    "\n",
    "#Now we create a new data column filled with random values and insert it into key 'x'\n",
    "\n",
    "xcolumn = np.random.uniform(20950, 21000, size=len(fcolumn))\n",
    "fdata = fdata._put_column(xcolumn,'x')\n",
    "\n",
    "#Asign some variable name and unit to the new column and plot the new data set\n",
    "\n",
    "fdata.header['col-x'] = 'Random'\n",
    "fdata.header['unit-col-x'] = 'arbitrary'\n",
    "mp.tsplot(fdata, height=2)\n",
    "\n",
    "#Other possibly commands to move, copy or drop individual columns are as follwos\n",
    "\n",
    "fdata = fdata._copy_column('x','var1')\n",
    "fdata = fdata._move_column('var1','var2')\n",
    "fdata = fdata._drop_column('var2')\n",
    "\n",
    "#Creating a data set with selected keys can also be accomplished by\n",
    "\n",
    "fdata = fdata._select_keys(['f'])\n",
    "\n",
    "#If you want to extract data by some given threshold values i.e. get only data exceeding a certain value you should\n",
    "#have a look at the `extract` method. The extract method requires three parameters: the first one defining the column/key name\n",
    "#the second the threshold value, and the third one defines a comparator, which can be one of \">=\", \"<=\",\">\", \"<\", \"==\", \"!=\",\n",
    "#default is \"==\". You can also apply the extract method on columns containing strings but then only with \"==\".\n",
    "\n",
    "extdata = fdata.extract(\"f\" , 48625, \">\")\n",
    "\n",
    "#Columns consisting solely of NaN values con be dropped using\n",
    "\n",
    "fdata = fdata._remove_nancolumns()\n",
    "\n",
    "#A random subselection of data can be obtained using `randomdrop`. The percentage defines the amount of data to be reomved.\n",
    "#You can also define indicies which cannot be randomly dropped, the first and last point in outr example below.\n",
    "\n",
    "dropstream = data.randomdrop(percentage=50,fixed_indicies=[0,len(data)-1])\n",
    "\n",
    "\n",
    "#As an example and for later we will add a secondary time column with a time shift to fdata\n",
    "\n",
    "tcolumn = fdata._get_column('time')\n",
    "newtcolumn = np.asarray([element+timedelta(minutes=15) for element in tcolumn])\n",
    "fdata = fdata._put_column(newtcolumn,'sectime')\n",
    "print (fdata.variables())\n",
    "\n",
    "\n",
    "#### 5.1.3 All about time\n",
    "\n",
    "# To extract time constrains use the following methods:\n",
    "# Covered time range\n",
    "\n",
    "print (data.timerange())\n",
    "print (data.start())\n",
    "print (data.end())\n",
    "\n",
    "# The sampling period in seconds can be obtained as follows\n",
    "print (data.samplingrate())\n",
    "\n",
    "#Whenever you load data sets with MagPy the data will be sorted according to the primary time column\n",
    "#You can manually repeat that anytime using\n",
    "\n",
    "data = data.sorting()\n",
    "\n",
    "#If you want to select specific time ranges from the already opened data set you can use the `trim`method\n",
    "\n",
    "trimmeddata = data.trim(starttime='2018-08-02T08:00:00', endtime='2018-08-02T09:00:00')\n",
    "print(\" Timesteps after trimming:\", len(trimmeddata))\n",
    "\n",
    "#The trim method will create a new datastream containing only data from the selected time window. There is another\n",
    "#mainly internally used method `_select_timerange` which will do exatcly the same as trim but returns only the\n",
    "#data array (ndarray) without any header information \n",
    "\n",
    "ar = data._select_timerange(starttime='2018-08-02T08:00:00', endtime='2018-08-02T09:00:00')\n",
    "print(\" Datatype after select_timerange:\", type(ar))\n",
    "print(\" Timesteps after _select_timerange:\", len(ar[0]))\n",
    "\n",
    "#Inversly you can drop a certain time range out of the data set by\n",
    "\n",
    "ddata = data.remove(starttime='2018-08-02T08:00:00', endtime='2018-08-02T09:00:00')\n",
    "\n",
    "#Please note that the remove command currently removes one point before starttime and endtime. This behavior\n",
    "#will be corrected in a future version.\n",
    "\n",
    "#Finally you can trim the given stream also by percentage or amount. This is done using the `cut`method and its\n",
    "#options. By default `cut` is using percentage. The following command will cutout the last 50% of data\n",
    "\n",
    "cutdata = fdata.cut(50,kind=0,order=0)\n",
    "print(cutdata.timerange())\n",
    "\n",
    "#Choosing option kind=1 will switch from percentage to amount and order=1 will take data from the beginning \n",
    "#of the data set\n",
    "\n",
    "cutdata = fdata.cut(10,kind=1,order=0)\n",
    "print(cutdata.timerange())\n",
    "\n",
    "#The default key list of any MagPy data stream supports two time columns 'time' and 'sec_time'. The secondary time column\n",
    "#might be used to store an alternative time reading i.e. GPS dates in the primary columns and NTP time in the secondary\n",
    "#one. You can switch this columns using a single command.\n",
    "\n",
    "shifted_fdata = fdata.use_sectime()\n",
    "\n",
    "#If you want to get the line index number of a specific time step in your data series you can get it by\n",
    "\n",
    "index = fdata.findtime(\"2018-08-02T22:22:22\")\n",
    "print(index)\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4d57092f-2902-4865-b6c5-331e4e31c2cd",
   "metadata": {},
   "outputs": [],
   "source": [
    "# 5.2 Coordinate transformation and rotations\n",
    "# ----------------------\n",
    "\n",
    "data = read(example5)\n",
    "\n",
    "#### 5.2.1 Rotations\n",
    "\n",
    "#Lets first look at our example data set. The example is provided in HEZ components. In order to convert HEZ into XYZ components we\n",
    "#simply need to rotate the data set by the declination value. You can achieve that by using the `rotation` method\n",
    "\n",
    "xyzdata = data.rotation(alpha=4.3)\n",
    "xyzdata.header['DataComponents'] = 'XYZ'\n",
    "xyzdata.header['col-x'] = 'X'\n",
    "xyzdata.header['col-y'] = 'Y'\n",
    "xyzdata.header['col-z'] = 'Z'\n",
    "mp.tsplot(xyzdata, height=2)\n",
    "\n",
    "#Vectorial data can be rotated using the `rotation` method by keeping F constant. Such rotations require just two angles which are referred to as *alpha* for rotations\n",
    "#in the declination plane and *beta* for rotations in the inclination plane. Assume a simple vector x=4, y=0 and z=4. Rotation by alpha=45° will lead to x=2,y=2,z=4, a clockwise rotation.\n",
    "#Rotating by beta=45° will rotate F into the X-Y plane with x=4, y=4, z=0. Please note: you need to supply xyz data when applying the `rotation` method.\n",
    "\n",
    "rotdata = data.rotation(alpha=45,beta=45)\n",
    "\n",
    "#### 5.2.2 Transforming coordinate systems\n",
    "\n",
    "#Assuming vector data in columns x,y,z you can freely convert between cartesian *xyz*, cylindrical *hdz*, and spherical *idf* coordinates:\n",
    "\n",
    "hdzdata = xyzdata.xyz2hdz()\n",
    "mp.tsplot(hdzdata, height=2)\n",
    "\n",
    "#The summary method `_convertstream` can also be used by giving the conversion type as option. The following conversions are possible: 'xyz2hdz','hdz2xyz','xyz2idf','idf2xyz':\n",
    "\n",
    "xyzdata = hdzdata._convertstream('hdz2xyz')\n",
    "mp.tsplot(xyzdata, height=2)\n",
    "\n",
    "#The vectorial data columns as defined by the keys 'x','y','z' need to filled accordingly. i.e. a XYZ data stream has X in key 'x', Y in key 'y' and Z in key 'z', all with the same unit, usually nT in magnetism.\n",
    "#A HDZ data stream as H assigned to key 'x', D in key 'y' and Z in key 'z'. H and Z are provided with the same unit (nT) and D has to be provided in degrees.decimals.\n",
    "#IDF data contains I in key 'x', D in key 'y' and F in key 'z', with I and D provided in degree.decimals.\n",
    "\n",
    "\n",
    "#### 5.2.3 Determining rotation angles\n",
    "\n",
    "#If you have a measurement (XYZ data) and would like to obtain the rotation values regarding and expected reference direction defined by *referenceD* reference declination and\n",
    "#*referenceI* inclination, both given in degree.decimals you can use the following method. Let us apply this method to the original rotdata, which\n",
    "# contains the HEZ data set rotated by alpha and beta of 45 degree. The HEZ data has an expected decliation of 0 and and expected inclination\n",
    "# of 64.4 degree. Please note that these values are not exact:\n",
    "\n",
    "alpha, beta = rotdata.determine_rotationangles(referenceD=0.0,referenceI=64.4)\n",
    "print (alpha, beta)\n",
    "\n",
    "# The method will return angles with which rotdata needs to be rotated in order to get a non-rotated data set. Thus alpha and beta of -45 degree \n",
    "# will be obtained.\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "05087eb0-7f61-4391-873f-a2da40ce8f6f",
   "metadata": {},
   "outputs": [],
   "source": [
    "# 5.3 Filtering and smoothing data\n",
    "# ----------------------\n",
    "\n",
    "data = read(example5)\n",
    "\n",
    "#### 5.3.1 General filter options and resampling\n",
    "\n",
    "#MagPy's `filter` uses the settings recommended by [IAGA]/[INTERMAGNET]. Ckeck `help(data.filter)` for further options \n",
    "#and definitions of filter types and pass bands. Here is short list of supported filter types:\n",
    "#'flat','barthann','bartlett','blackman','blackmanharris','bohman','boxcar',\n",
    "#'cosine','flattop','hamming','hann','nuttall','parzen','triang','gaussian','wiener','butterworth'\n",
    "#Important options of the filter method, beside the chosen *filter_type* are *filter_width* which defines the window width\n",
    "#in a timedelta object and the *resample_period* in seconds defining the resolution of the resulting data set.\n",
    "#To get an overview over all filter_type and their basic characteristics you can run the following code. This will\n",
    "#calculate filtered one-minute data and then calculate and plot the power spectral density of each filtered data set.\n",
    "#Please note that we apply the *missingdata='interpolate'* option as the matplotlib.pyplot.psd method requires the data set\n",
    "#being free of NaN values.\n",
    "\n",
    "filterlist = ['flat', 'barthann', 'bartlett', 'blackman', 'blackmanharris', 'bohman',\n",
    "                              'boxcar', 'cosine', 'flattop', 'hamming', 'hann', 'nuttall', 'parzen', 'triang',\n",
    "                              'gaussian', 'wiener', 'butterworth']\n",
    "for filter_type in filterlist:\n",
    "    filtereddata = data.filter(filter_type=filter_type, missingdata='interpolate', filter_width=timedelta(seconds=120), resample_period=60)\n",
    "    T = filtereddata._get_column('time')\n",
    "    t = np.linspace(0,len(T),len(T))\n",
    "    h = filtereddata._get_column('x') - filtereddata.mean('x')\n",
    "    sr = filtereddata.samplingrate() # in seconds\n",
    "    fs = 1./sr\n",
    "    fig, (ax0, ax1) = plt.subplots(2, 1, layout='constrained')\n",
    "    ax0.plot(t, h)\n",
    "    ax0.set_xlabel('Time')\n",
    "    ax0.set_ylabel('Signal')\n",
    "    ax1.set_title(filter_type)\n",
    "    power, freqs = ax1.psd(h, NFFT=len(t), pad_to=len(t), Fs=fs, scale_by_freq=True)\n",
    "    plt.show()\n",
    "\n",
    "#The `filter` method will resample the data set by default towards a projected period. In order to skip resampling choose option\n",
    "#*noresample=True*. You can apply the resample method also to any data set as follows. Resample will extract values at the given\n",
    "#sampling interval or take the linear interpolated value between adjacent data points if no value is existing at the given time\n",
    "#step.\n",
    "\n",
    "resampleddata = data.resample(['x','y','z'],period=60)\n",
    "print(len(resampleddata))\n",
    "print(resampleddata.timerange())\n",
    "\n",
    "\n",
    "#### 5.3.2 Smoothing data\n",
    "\n",
    "#The `smooth` method is similar to the filter method without resampling. It is a quick method with only a limited amount \n",
    "#of supported window types: flat, hanning (default), hamming, bartlett and blackman. The window length is given as number\n",
    "#of data points. Smooth will throw an error in case of columns with only NaN values, so make sure to drop them before\n",
    "\n",
    "data = data._remove_nancolumns()\n",
    "smootheddata = data.smooth(window='hanning', window_len=11)\n",
    "\n",
    "#### 5.3.3 Filtering in geomagnetic applications\n",
    "\n",
    "#When dealing with geomagnetic data a number of simplifications have been added into the application. To filter the data \n",
    "#set with default parameters as recommended by IAGA you can skip all the options and just call `filter`.\n",
    "#It automatically chooses a gaussian window with the correct settings depending on the provided sampling rate of the data \n",
    "#set and filter towards the next time period i.e. if you supply 1sec, 5 sec or 10 sec period data they will be filtered to 1 min.\n",
    "#Therefore in basically all geomagnetic applications the following command is sufficient\n",
    "\n",
    "filtereddata = data.filter()\n",
    "\n",
    "#Get sampling rate and filtered data after filtering (please note that all filter information is added to the data's meta information dictionary (data.header):\n",
    "\n",
    "print(\"Sampling rate after [sec]:\", filtereddata.samplingrate())\n",
    "print(\"Filter and pass band:\", filtereddata.header.get('DataSamplingFilter',''))\n",
    "\n",
    "#### 5.3.4 Frequnecy analysis and missing data\n",
    "\n",
    "#When dealing with geomagnetic data, especially when it comes to the frequency domain, then the treatment of missing data and\n",
    "#is of particluar importance. The time domain should be evenly distributed and misisng data needs to be adequatly \n",
    "#considered. Missing data is often replaced by unrealistic numerical values like 99999 or negative data. MagPy is using \n",
    "#NaN values instead to internally treat such missing data. Two methods are available to help you preparing your data\n",
    "#for frequency analysis. The `get_gaps` method (section 5.9.2) will analyse the time column and identify any missing time steps for an \n",
    "#equally distant time scale. Time steps will be filled in and NaNs will be added into the data columns. In order to deal \n",
    "#with NaN values you can use filtering procedures as shown above or use the `interpolate_nan` method (section 5.9.2), \n",
    "#which will linearly interpolate NaNs. Please be careful with these techniques as you might create spurious signals when interpolating. \n",
    "\n",
    "data = data.get_gaps()\n",
    "data = data.interpolate_nans(['x'])\n",
    "\n",
    "#The power spectral density can then be analyzed using build in python matplotlib methods. First extract time and data column.\n",
    "\n",
    "T = data._get_column('time')\n",
    "print (len(T))\n",
    "t = np.linspace(0,len(T),len(T))\n",
    "h = data._get_column('x')\n",
    "sr = data.samplingrate() # in seconds\n",
    "fs = 1./sr\n",
    "\n",
    "#Then plot timeseries of the selected data and psd.\n",
    "\n",
    "fig, (ax0, ax1) = plt.subplots(2, 1, layout='constrained')\n",
    "ax0.plot(t, h)\n",
    "ax0.set_xlabel('Time')\n",
    "ax0.set_ylabel('Signal')\n",
    "power, freqs = ax1.psd(h, NFFT=len(t), pad_to=len(t), Fs=fs, detrend='mean', scale_by_freq=True)\n",
    "plt.show()\n",
    "\n",
    "\n",
    "#### 5.3.4 Quickly get daily mean values\n",
    "\n",
    "#Another method which belongs basically to the filter section is the the `dailymeans` method which allow you to \n",
    "#quickly obtain dailymean values according to IAGA standards from any given data set covering at least one day.\n",
    "#Acceptable sind all data resolutions as the dailymeans will filter the data stepwise until daily mean values.\n",
    "\n",
    "dailymeans = data.dailymeans()\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c0fb8410-5cbb-4b7c-9c08-2c79ebf0e26b",
   "metadata": {},
   "outputs": [],
   "source": [
    "# 5.4 Calculating vectorial F and delta F\n",
    "# ----------------------\n",
    "\n",
    "#Vectorial F can be easily calculated if the vectorial keys x,y,z are available. Lets start with examples1\n",
    "#from the provided data sets. Example1 contains a column filled with nan values for testing purposes.\n",
    "#We first remove this column (see also 5.1.2). \n",
    "\n",
    "data = read(example1)\n",
    "data = data._remove_nancolumns()\n",
    "\n",
    "#Afterwards we check the available keys in the data set and see that x,y,z \n",
    "#are available, a prerequisite to calculate the vector sum. We can also check what components are actually \n",
    "#stored below keys x,y,z by checking the data's meta information. HEZ is perfectly fine for calculating\n",
    "#the vector sum.\n",
    "\n",
    "print(data.variables())\n",
    "print(data.header.get('DataComponents'))\n",
    "\n",
    "#The command 'calc_f` is now performing the calculation of the vector sum and stores it with key f\n",
    "\n",
    "data_with_F_v = data.calc_f()\n",
    "\n",
    "#Mostly however you will be interested not in vectorial F (F_V) but in delta values between F_V and a scalar F (F_S).\n",
    "#Lets read an independent F data set from example2, which covers a similar time range as vectorial data from example1.\n",
    "\n",
    "fdata = read(example2)\n",
    "\n",
    "#Let us assume you have two data sources variodata with X,Y,and Z data as well as scalardata with F. \n",
    "#Make sure that both data sets cover the same time range and are sampled at the same frequency and time steps\n",
    "\n",
    "combineddata = merge_streams(data,fdata)   # checkout section 5.10 for details\n",
    "\n",
    "#Now the data file contains xyz (hdz, idf) data and an independently measured f value. You can calculate delta F \n",
    "#between the two instruments using the following:\n",
    "\n",
    "combineddata = combineddata.delta_f()\n",
    "\n",
    "#Combined data will now contain an additional column at key 'df' containing F_v - F_s, \n",
    "#the scalar pier difference as defined within the IM technical manual. The delta F values will added \n",
    "#to key/column df:\n",
    "\n",
    "mp.tsplot(combineddata, keys=['x','y','z','f','df'], height=2)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5f6dfd42-5c72-4350-a039-d6cc35ca831e",
   "metadata": {},
   "outputs": [],
   "source": [
    "# 5.5 Mean, amplitudes and standard deviation\n",
    "# ----------------------\n",
    "\n",
    "data = read(example1)\n",
    "\n",
    "#Mean values for certain data columns can be obtained using the `mean` method. The mean will only be \n",
    "#calculated for data with the percentage of valid data points. By default 95% of vaild data is required. \n",
    "#You can change that by using the percentage option. In case of too many missing data points, then no mean \n",
    "#is calulated and the function returns NaN.\n",
    "\n",
    "print(data.mean('x', percentage=80))\n",
    "\n",
    "#If you want also the standard deviation use option *std*:\n",
    "\n",
    "print(data.mean('x', percentage=80, std=True))\n",
    "\n",
    "#The median can be calculated by defining the `meanfunction` option:\n",
    "\n",
    "print(data.mean('x', meanfunction='median'))\n",
    "\n",
    "#The amplitude, the difference between maximum and minimum, can be obtained as follows\n",
    "\n",
    "print(data.amplitude('x'))\n",
    "\n",
    "#Just maximum and minimum values can be obtained with these methods\n",
    "\n",
    "print(\"Maximum:\", data._get_max('x'))\n",
    "print(\"Minimum:\", data._get_min('x'))\n",
    "\n",
    "#If you just need the variance you can either square the standrad deviation or use the `_get_variance` method\n",
    "\n",
    "print(\"Variance:\", data._get_variance('x'))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "13537a64-f823-4d8e-b403-0f12c323d1dd",
   "metadata": {},
   "outputs": [],
   "source": [
    "# 5.6 Offsets and Scales\n",
    "# ----------------------\n",
    "\n",
    "data = read(example1)\n",
    "\n",
    "#### 5.6.1 Offsets\n",
    "\n",
    "#Constant offsets can be added to individual columns using the `offset` method with a dictionary defining \n",
    "#the MagPy stream column keys and the offset to be applied (datetime.timedelta object for time column, float for all others):\n",
    "\n",
    "offsetdata = data.offset({'time':timedelta(seconds=0.19),'f':1.24})\n",
    "\n",
    "#### 5.6.2 Scaling\n",
    "\n",
    "#Individual columns can also be multiplied by values provided in a dictionary:\n",
    "\n",
    "multdata = data.multiply({'x':-1})\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "31199812-ef58-4b68-8417-f0c75e9a7657",
   "metadata": {},
   "outputs": [],
   "source": [
    "# 5.7 Derivatives and integrating\n",
    "# ----------------------\n",
    "\n",
    "data = read(example1)\n",
    "\n",
    "#Time derivatives, which are useful to identify outliers and sharp changes, are calculated based on successive\n",
    "#gradients based on numpy gradient. By using the option *put2keys* you can add the derivative to columns of your\n",
    "#choice. By default they are added to dx,,dy, dz, df.\n",
    "\n",
    "diffdata = data.derivative(keys=['x','y','z'],put2keys = ['dx','dy','dz'])\n",
    "mp.tsplot(diffdata,keys=['x','dx'], height=2)\n",
    "\n",
    "#We can also integrate the curve again based on scipy.integrate.cumtrapz. Use the `integrate`method for this purpose.\n",
    "#Integrate can only be applied to keys x,y,z,f and puts integrated data into columns dx,dy,dz,df. So sometimes you will\n",
    "#need to move data into the projected columns first. In the following we will move one of the earlier derivated\n",
    "#columns towards x and then integrate. The correct scaling cannot be reconstructed and needs to be adjusted separetly\n",
    "\n",
    "diffdata._move_column('dx','x')\n",
    "test = diffdata.integrate(keys=['x','y','z'])\n",
    "mp.tsplot(test,keys=['dx'], height=2)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ab0ed981-5664-4ac9-af99-006a0462e450",
   "metadata": {},
   "outputs": [],
   "source": [
    "# 5.8 Extrapolation\n",
    "# ----------------------\n",
    "\n",
    "#The extrapolation method `extrapolate` allows to extrapolate a  data set towards given start and end times. Several \n",
    "#different methods are available for extrapolation: The most simple extrapolation method, which was already available\n",
    "#in MagPy 1.x is the duplication method (option: *method='old'*) which duplicates the first and last existing points \n",
    "#at given times. New methods starting form 2.0 are the *'spline'* technique following \n",
    "#[this](https://docs.scipy.org/doc/scipy/tutorial/interpolate/extrapolation_examples.html) approach, a *'linear'* \n",
    "#extrapolation and a *'fourier'* technique as described [here](https://gist.github.com/tartakynov/83f3cd8f44208a1856ce).\n",
    "#Please note: the extrapolation method will remove any NaN columns and secondary time columns.\n",
    "\n",
    "data = read(example5)\n",
    "shortdata = data.trim(starttime='2018-08-29T09:00:00', endtime='2018-08-29T14:00:00')\n",
    "extdata = shortdata.extrapolate(starttime=datetime(2018,8,29,7), endtime=datetime(2018,8,29,16), method='fourier')\n",
    "\n",
    "#The different techniques will result in schemes as displayed in the following diagrams (not the example above):\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "019bb732-bfe1-4a3a-bd21-a8e2de98db2a",
   "metadata": {},
   "outputs": [],
   "source": [
    "# 5.9 Functions\n",
    "# ----------------------\n",
    "\n",
    "data = read(example5)\n",
    "\n",
    "#### 5.9.1 Fitting data\n",
    "\n",
    "#MagPy offers the possibility to fit functions to data using a number of different fitting functions:\n",
    "\n",
    "func = data.fit(keys=['x','y','z'], fitfunc='spline', knotstep=0.1)\n",
    "mp.tsplot([data],[['x','y','z']],functions=[[func,func,func]])\n",
    "\n",
    "#Supported fitting functions *fitfunc* are polynomial 'poly', 'harmonic', 'least-squares', 'mean', 'spline'. The default \n",
    "#fitting method is the cubic spline function 'spline'. You need to specific the option *fitdegree* for polynomial and \n",
    "#harmonic fitting functions. *fitdegree*=1 corresponds to a linear fit. Default value is 5. For *fitfunc*='spline' you \n",
    "#need to specify an average spacing for knots. The *knotstep* parameter will define at which percental distance a knot \n",
    "#should be located. i.e. *knotstep*=0.33 would place altogether 2 knots at 33% within the timeseries. Smaller values \n",
    "#will increase the number of knots and thus the complexity of the fit. Thus, *knotstep* need to contain a positive \n",
    "#number below 0.5.\n",
    "\n",
    "#### 5.9.2 Interpolation\n",
    "\n",
    "#The interpol method uses Numpy's interpolate.interp1d to interpolate values of a timeseries. The option *kind* defines \n",
    "#the type of interpolation. Possible options are 'linear' (default), 'slinear ' which is a first order spline, \n",
    "#'quadratic' = spline (second order), 'cubic' corresponding to a third order spline, 'nearest' values and 'zero'. The \n",
    "#interpolation method can be used to interpolate missing data. Lets create put some data gaps into our example data set\n",
    "#for demonstration. We will use randomdrop to remove some data lines and then use `get_gaps` to identify this lines and\n",
    "#insert timesteps, but leaving values as NaN. Finally we can then interpolate missing data \n",
    "\n",
    "discontinuousdata_with_gaps = data.randomdrop(percentage=10,fixed_indicies=[0,len(data)-1])\n",
    "print(\"Before: {}, After randdrop: {}\".format(len(data), len(discontinuousdata_with_gaps)))\n",
    "continuousdata_with_gaps = discontinuousdata_with_gaps.get_gaps()\n",
    "\n",
    "contfunc = continuousdata_with_gaps.interpol(['x','y'],kind='linear')\n",
    "mp.tsplot([continuousdata_with_gaps],[['x','y','z']],functions=[[contfunc,contfunc,None]])\n",
    "\n",
    "#Another simple interpolation method allows for a quick linear interpolation of values, directly modifying the supplied \n",
    "#timeseries (see also section 5.3.4).\n",
    "\n",
    "interpolatedts = continuousdata_with_gaps.interpolate_nans(['f'])\n",
    "\n",
    "#### 5.9.3 Adopted baselines\n",
    "\n",
    "#Baselines are also treated as functions in MagPy. You can calculate the adopted baseline as follows. A more detailed \n",
    "#description, also highlighting options for adopted baseline functions and jumps and all other aspects of\n",
    "#adopted baseline fitting are given in section 7.5. The baseline method will add functional parameter and basevalues\n",
    "#into the data header of variodata. You find these meta information in header keys 'DataAbsInfo' and\n",
    "#'DataBaseValues'. 'DataAbsInfo' contains the functional parameters of which the first two elements of each\n",
    "# function list describe the time range given in numerical matplotlib.dates.\n",
    "\n",
    "variodata = read(example5)\n",
    "basevalues = read(example3)\n",
    "func = variodata.baseline(basevalues)\n",
    "\n",
    "#### 5.9.4 Functions within a DataStream object\n",
    "\n",
    "#The full function objects can be added to the timeseries meta information dictionary and stored along with the data set. Such Object \n",
    "#storage is only supported for MagPy's PYCDF format. To add functions into the timeseries data header use:\n",
    "\n",
    "variodata = variodata.func2header(func)\n",
    "\n",
    "#When reading PYCDF data files and also INTERMAGNET IBLV data files then functional values (adopted baselines of BLV \n",
    "#files) are available in the header. Access it as follows:\n",
    "\n",
    "blvdata = read(example7)\n",
    "func = blvdata.header.get('DataFunctionObject')\n",
    "mp.tsplot([blvdata],[['dx','dy','dz']], symbols=[['.','.','.']], padding=[[0.005,0.005,2]], colors=[[[0.5, 0.5, 0.5],[0.5, 0.5, 0.5],[0.5, 0.5, 0.5]]], functions=[[func,func,func]], height=2)\n",
    "\n",
    "\n",
    "#### 5.9.5 Applying functions to timeseries\n",
    "\n",
    "#Functions can be transferred to data values and they can be subtracted for residual analysis. Use method func2stream \n",
    "#for this purpose. You need to supply the functions to func2stream, define the keys and a mode on how functions are \n",
    "#applied to the new timeseries. Possible modes are 'add', 'sub' for subtracting, 'div' for division, 'multiply' and\n",
    "#'values' to replace any existing data by function values. In order to analyse residuals for a adopted baseline function \n",
    "#you would do the following: \n",
    "\n",
    "residuals = blvdata.copy()\n",
    "residuals = residuals.func2stream(func, keys=['dx','dy','dz','df'],mode='sub')\n",
    "print (\" Get the average residual value:\", residuals.mean('dy',percentage=90))\n",
    "mp.tsplot([residuals],[['dx','dy','dz']], symbols=[['.','.','.']], height=2)\n",
    "\n",
    "#Replacing existing data by interpolated values can be accomplished using the `func2stream` method. Just recall the example\n",
    "#of 5.9.2, where we filled gaps by interpolation. Know we replace all existing inputs (data and NaNs) \n",
    "#by interpolated values\n",
    "\n",
    "contfunc = continuousdata_with_gaps.interpol(['x','y','z'],kind='linear')\n",
    "data = data.func2stream(contfunc, keys=['x','y','z'],mode='values')\n",
    "\n",
    "#### 5.9.6 Saving and reading functions separatly\n",
    "\n",
    "#It is possible to save the functional parameters (NOT the functions) to a file and reload them for later usage. Please \n",
    "#note that you will need to apply the desired fit/interpolation/baseline adoption again based on these parameters to \n",
    "#obtain a function object. The parameters will be stored within a json dictionary.\n",
    "\n",
    "func_to_file(contfunc, \"/tmp/savedparameter.json\")\n",
    "\n",
    "#To read parameters in again \n",
    "\n",
    "funcparameter = func_from_file(\"/tmp/savedparameter.json\")\n",
    "\n",
    "#The variable funcparameters will then contain a dictionary with all contents of the original function list, including \n",
    "#time ranges and specific parameters for each value. Extract these values by standard dict.get() and reapply to the data \n",
    "#stream. \n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "13001774-ff84-4612-8daa-534a03ac1d5f",
   "metadata": {},
   "outputs": [],
   "source": [
    "# 5.10 Multiple timeseries\n",
    "# ----------------------\n",
    "\n",
    "def create_minteststream(startdate=datetime(2022, 11, 1), addnan=True):\n",
    "        c = 1000  # 4000 nan values are filled at random places to get some significant data gaps\n",
    "        l = 32 * 1440\n",
    "        #import scipy\n",
    "        teststream = DataStream()\n",
    "        array = [[] for el in DataStream().KEYLIST]\n",
    "        win = signal.windows.hann(60)\n",
    "        a = np.random.uniform(20950, 21000, size=int(l / 2))\n",
    "        b = np.random.uniform(20950, 21050, size=int(l / 2))\n",
    "        x = signal.convolve(np.concatenate([a, b], axis=0), win, mode='same') / sum(win)\n",
    "        if addnan:\n",
    "            x.ravel()[np.random.choice(x.size, c, replace=False)] = np.nan\n",
    "        array[1] = x[1440:-1440]\n",
    "        a = np.random.uniform(1950, 2000, size=int(l / 2))\n",
    "        b = np.random.uniform(1900, 2050, size=int(l / 2))\n",
    "        y = signal.convolve(np.concatenate([a, b], axis=0), win, mode='same') / sum(win)\n",
    "        if addnan:\n",
    "            y.ravel()[np.random.choice(y.size, c, replace=False)] = np.nan\n",
    "        array[2] = y[1440:-1440]\n",
    "        a = np.random.uniform(44300, 44400, size=l)\n",
    "        z = signal.convolve(a, win, mode='same') / sum(win)\n",
    "        array[3] = z[1440:-1440]\n",
    "        array[4] = np.sqrt((x * x) + (y * y) + (z * z))[1440:-1440]\n",
    "        array[0] = np.asarray([startdate + timedelta(minutes=i) for i in range(0, len(array[1]))])\n",
    "        array[KEYLIST.index('sectime')] = np.asarray(\n",
    "            [startdate + timedelta(minutes=i) for i in range(0, len(array[1]))]) + timedelta(minutes=15)\n",
    "        teststream = DataStream(header={'SensorID': 'Test_0001_0001'}, ndarray=np.asarray(array, dtype=object))\n",
    "        minstream = teststream.filter()\n",
    "        teststream.header['col-x'] = 'X'\n",
    "        teststream.header['col-y'] = 'Y'\n",
    "        teststream.header['col-z'] = 'Z'\n",
    "        teststream.header['col-f'] = 'F'\n",
    "        teststream.header['unit-col-x'] = 'nT'\n",
    "        teststream.header['unit-col-y'] = 'nT'\n",
    "        teststream.header['unit-col-z'] = 'nT'\n",
    "        teststream.header['unit-col-f'] = 'nT'\n",
    "        return teststream\n",
    "\n",
    "\n",
    "data1 = create_minteststream(startdate=datetime(2022, 11, 1))\n",
    "data2 = create_minteststream(startdate=datetime(2022, 11, 15))\n",
    "data1.ndarray[1][31000:33000] = np.nan\n",
    "data2.ndarray[1][5000:9000] = np.nan\n",
    "\n",
    "# Join\n",
    "join1 = join_streams(data1,data2)\n",
    "p1 = mp.tsplot([data2,data1],[['x','f']],colors=[[[0.8, 0.8, 0.8],[0.8, 0.8, 0.8]],[[0.5, 0.5, 0.5],[0.5, 0.5, 0.5]]], legend={\"legendtext\" : [\"data2\", \"data1\"]}, height=2)\n",
    "#p1.savefig(\"./ms_data.png\")\n",
    "p2 = mp.tsplot([join1],[['x','f']],colors=[[[0.2, 0.2, 0.2],[0.2, 0.2, 0.2]]], title=\"joined\", height=2)\n",
    "join2 = join_streams(data2,data1)\n",
    "p1 = mp.tsplot([data2,data1],[['x','f']],colors=[[[0.8, 0.8, 0.8],[0.8, 0.8, 0.8]],[[0.5, 0.5, 0.5],[0.5, 0.5, 0.5]]], legend={\"legendtext\" : [\"data2\", \"data1\"]}, height=2)\n",
    "#p1.savefig(\"./ms_data.png\")\n",
    "p2 = mp.tsplot([join2],[['x','f']],colors=[[[0.2, 0.2, 0.2],[0.2, 0.2, 0.2]]], title=\"joined\", height=2)\n",
    "\n",
    "# Merge\n",
    "merge1 = merge_streams(data1,data2)\n",
    "p1 = mp.tsplot([data2,data1],[['x','f']],colors=[[[0.8, 0.8, 0.8],[0.8, 0.8, 0.8]],[[0.5, 0.5, 0.5],[0.5, 0.5, 0.5]]], legend={\"legendtext\" : [\"data2\", \"data1\"]}, height=2)\n",
    "#p1.savefig(\"./ms_data.png\")\n",
    "p2 = mp.tsplot([merge1],[['x','f']],colors=[[[0.2, 0.2, 0.2],[0.2, 0.2, 0.2]]], title=\"merged\", height=2)\n",
    "#p2.savefig(\"./ms_merge.png\")\n",
    "merge2 = merge_streams(data2,data1, keys=['x'])\n",
    "p = mp.tsplot([data2,data1],[['x','f']],colors=[[[0.8, 0.8, 0.8],[0.8, 0.8, 0.8]],[[0.5, 0.5, 0.5],[0.5, 0.5, 0.5]]], legend={\"legendtext\" : [\"data2\", \"data1\"]}, height=2)\n",
    "p3 = mp.tsplot([merge2],[['x']],colors=[[[0.2, 0.2, 0.2]]], title=\"merged\", height=2)\n",
    "\n",
    "# Subtract\n",
    "sub1 = subtract_streams(data1,data2)\n",
    "p1 = mp.tsplot([data2,data1],[['x','f']],colors=[[[0.8, 0.8, 0.8],[0.8, 0.8, 0.8]],[[0.5, 0.5, 0.5],[0.5, 0.5, 0.5]]], legend={\"legendtext\" : [\"data2\", \"data1\"]}, height=2)\n",
    "#p1.savefig(\"./ms_data.png\")\n",
    "p2 = mp.tsplot([sub1],[['x','f']],colors=[[[0.2, 0.2, 0.2],[0.2, 0.2, 0.2]]], title=\"subtract\", height=2)\n",
    "#p2.savefig(\"./ms_merge.png\")\n",
    "\n",
    "# Time shifts between two data sets\n",
    "shifted_stream = data1.copy()\n",
    "shifted_stream = shifted_stream.use_sectime()\n",
    "data1 = data1.trim(\"2022-11-22T00:00:00\",\"2022-11-23T00:00:00\")\n",
    "shifted_stream = shifted_stream.trim(\"2022-11-22T00:00:00\",\"2022-11-23T00:00:00\")\n",
    "\n",
    "p1 = mp.tsplot([data1,shifted_stream],[['x','f']],colors=[[[0.8, 0.8, 0.8],[0.8, 0.8, 0.8]],[[0.5, 0.5, 0.5],[0.5, 0.5, 0.5]]], legend={\"legendtext\" : [\"data1\", \"shifted data1\"]}, height=2)\n",
    "\n",
    "shift = determine_time_shift(data1,shifted_stream, col2compare='f',method='all', debug=True)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "79746e6f-763b-4339-b915-08e840345f96",
   "metadata": {},
   "outputs": [],
   "source": [
    "# ---------------------------------------\n",
    "# Chapter 6: Annotating data and flagging\n",
    "# ---------------------------------------"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a456f282-7020-4ffa-9c24-b79afe5ff8cf",
   "metadata": {},
   "outputs": [],
   "source": [
    "# 6.1. Flagging basics\n",
    "# ----------------------\n",
    "import sys\n",
    "sys.path.insert(1,'/home/leon/Software/magpy/') # should be magpy2\n",
    "from magpy.core import flagging\n",
    "\n",
    "fl = flagging.Flags()\n",
    "\n",
    "#Add flags to this object\n",
    "\n",
    "fl = fl.add(sensorid=\"LEMI025_X56878_0002_0001\", starttime=\"2022-11-22T16:36:12.654362\",\n",
    "                    endtime=\"2022-11-22T16:41:12.654362\", components=['x', 'y', 'z'], labelid='020', flagtype=4,\n",
    "                    comment=\"SSC with an amplitude of 40 nT\", operator='John Doe')\n",
    "fl = fl.add(sensorid=\"GSM90_Y1112_0001\", starttime=\"2022-11-22T10:56:12.654362\",\n",
    "                          endtime=\"2022-11-22T10:59:12.654362\", components=['f'], flagtype=3, labelid='050')\n",
    "fl = fl.add(sensorid=\"GSM90_Y1112_0001\", starttime=\"2022-11-22T10:58:12.654362\",\n",
    "                          endtime=\"2022-11-22T11:09:45.654362\", components=['f'], flagtype=3, labelid='050')\n",
    "fl = fl.add(sensorid=\"LEMI025_X56878_0002_0001\", starttime=\"2022-11-22T20:59:13.0\",\n",
    "                    endtime=\"2022-11-22T20:59:14.0\", components=['x', 'y', 'z'], labelid='001', flagtype=3,\n",
    "                    operator='John Doe')\n",
    "\n",
    "#You can get a formated output of the flagging contents by \n",
    "\n",
    "fl.fprint(sensorid=\"LEMI025_X56878_0002_0001\")\n",
    "\n",
    "#You might want to limit the output to specific sensorids by providing the Sensor ID as option. If you want to modify \n",
    "#flags and keep the original state in the memory use the copy method\n",
    "\n",
    "orgfl = fl.copy()\n",
    "\n",
    "#A number of methods are available for modifying and extracting flagging information. You can extract specific\n",
    "#information for any content (field):\n",
    "\n",
    "lightningfl = fl.select('comment', ['lightning'])\n",
    "\n",
    "#You can also select a specific time range by using the trim method\n",
    "\n",
    "timefl = fl.trim(starttime='2022-11-22T09:00:00', endtime='2022-11-22T11:00:00')\n",
    "\n",
    "#Combination of different flagging objects is possible as follows\n",
    "\n",
    "comb = timefl.join(lightningfl)\n",
    "\n",
    "#Sometime you are interested in the differences of two flagging objects\n",
    "\n",
    "diff = comb.diff(fl)\n",
    "\n",
    "#Flags defined by a parameter (field) and searchterms for this parameter can be dropped from the flagging object\n",
    "\n",
    "newfl = fl.drop(parameter='sensorid', values=['GSM90_Y1112_0001'])\n",
    "\n",
    "#Specific contents associated to a given field can be replaced \n",
    "\n",
    "flmodified = fl.replace('stationid', '', 'WIC')\n",
    "\n",
    "#Time ranges of overlapping flags can be combined using the `union` method. \n",
    "\n",
    "combfl = fl.union(samplingrate=1, level=0)\n",
    "\n",
    "# Rename nearby data\n",
    "\n",
    "renamedfl = fl.rename_nearby(parameter='labelid', values=['001'])\n",
    "\n",
    "# some general information \n",
    "\n",
    "mintime, maxtime = fl.timerange()\n",
    "\n",
    "fl.stats(level=1)\n",
    "\n",
    "# if you want to connect and save flagging information together with data stream then add the flagging object \n",
    "# into the data header of the data stream\n",
    "\n",
    "data = DataStream()\n",
    "data.header[\"DataFlags\"] = fl\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e4a5d48c-fae7-4332-9533-ffa50b070655",
   "metadata": {},
   "outputs": [],
   "source": [
    "# 6.2. Flagging outliers\n",
    "# ----------------------\n",
    "import sys\n",
    "sys.path.insert(1,'/home/leon/Software/magpy/') # should be magpy2\n",
    "from magpy.stream import *\n",
    "from magpy.core import plot as mp\n",
    "from magpy.core import flagging\n",
    "%matplotlib widget\n",
    "\n",
    "datawithspikes = read(example1)\n",
    "datawithspikes = datawithspikes.trim(starttime=\"2018-08-02T14:30:00\",endtime=\"2018-08-02T15:30:00\")\n",
    "\n",
    "#Mark all spikes using the automated function `flag_outlier` with default options and return a flagging object:\n",
    "\n",
    "fl = flagging.flag_outlier(datawithspikes, timerange=60, threshold=1.5, markall=True)\n",
    "\n",
    "#Drop flagged data from the \"disturbed\" data set.\n",
    "\n",
    "datawithoutspikes = fl.apply_flags(datawithspikes)\n",
    "\n",
    "#Show original data in grey, cleand data in black and flagging ranges in a single plot:\n",
    "\n",
    "fig, ax = mp.tsplot([datawithspikes,datawithoutspikes], [['x','y','z']], colors=[['r','r','r'],['grey','grey','grey']], height = 2)\n",
    "\n",
    "fig.savefig(\"/tmp/fl_outlier.png\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c80225db-83c2-43a0-9f9c-cf54c30d2e02",
   "metadata": {},
   "outputs": [],
   "source": [
    "# 6.3. Flagging ranges\n",
    "# ----------------------\n",
    "import sys\n",
    "sys.path.insert(1,'/home/leon/Software/magpy/') # should be magpy2\n",
    "from magpy.stream import *\n",
    "from magpy.core import plot as mp\n",
    "from magpy.core import flagging\n",
    "%matplotlib widget\n",
    "\n",
    "#The load a data record and firstly add a flagging range in time.\n",
    "\n",
    "data = read(example1)\n",
    "\n",
    "timefl = flagging.flag_range(data, keys=['x'], \n",
    "                                     starttime='2018-08-02T14:30:00', \n",
    "                                     endtime='2018-08-02T15:15:00', \n",
    "                                     flagtype=3, \n",
    "                                     labelid='051', \n",
    "                                     text=\"iron maiden dancing near sensor\",\n",
    "                                     operator=\"Max Mustermann\")\n",
    "\n",
    "#We also will flag values exceeding a given threshold in the same data set:\n",
    "\n",
    "valuefl = flagging.flag_range(data, keys=['x'], \n",
    "                                  above=21067,\n",
    "                                  flagtype=0,\n",
    "                                  labelid='000',\n",
    "                                  text=\"interesting values for later discussion\",\n",
    "                                  operator=\"Mimi Musterfrau\")\n",
    "\n",
    "#For displaying put these flags together using the `join`method:\n",
    "\n",
    "fl = valuefl.join(timefl)\n",
    "\n",
    "#Create graphical patches for displaying this flagging information:\n",
    "\n",
    "p = fl.create_patch()\n",
    "\n",
    "#Show flagged data in a plot:\n",
    "\n",
    "fig, ax = mp.tsplot(data,['x'],patch=p, annotate='labelid', height=2)\n",
    "fig.savefig(\"/tmp/fl_range.png\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "73683734-a7e8-4af0-b104-08fc883937c0",
   "metadata": {},
   "outputs": [],
   "source": [
    "# 6.4. Flagging binary states\n",
    "# ----------------------\n",
    "import sys\n",
    "sys.path.insert(1,'/home/leon/Software/magpy/') # should be magpy2\n",
    "from magpy.stream import *\n",
    "from magpy.core import plot as mp\n",
    "from magpy.core import flagging\n",
    "%matplotlib widget\n",
    "\n",
    "#The load a data record and firstly add a flagging range in time.\n",
    "\n",
    "data = read(example1)\n",
    "\n",
    "# add somw artifical binary data i.e. a light switch\n",
    "\n",
    "var1 = [0]*len(data)\n",
    "var1[43200:50400] = [1]*7200\n",
    "data = data._put_column(np.asarray(var1), 'var1')\n",
    "\n",
    "# flag column x based on the switching state of column var1. By default status changes are flagged.\n",
    "# The option *markallon' will additionally mark the full time range containing 1 in var1.\n",
    "\n",
    "fl = flagging.flag_binary(data, key='var1', keystoflag=[\"x\"],\n",
    "                                     flagtype=3, \n",
    "                                     text=\"light switch affecting signal\",\n",
    "                                     markallon=True)\n",
    "\n",
    "#Create graphical patches for displaying this flagging information:\n",
    "\n",
    "p = fl.create_patch()\n",
    "\n",
    "#Show flagged data in a plot:\n",
    "\n",
    "fig, ax = mp.tsplot(data,['x'],patch=p, height=2)\n",
    "#fig.savefig(\"/home/leon/Downloads/fl_binary.png\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "34122630-6cc9-4058-9808-1e5f2691dace",
   "metadata": {},
   "outputs": [],
   "source": [
    "# 6.5. Converting stream data to flagging information\n",
    "# ----------------------\n",
    "import sys\n",
    "sys.path.insert(1,'/home/leon/Software/magpy/') # should be magpy2\n",
    "from magpy.stream import *\n",
    "from magpy.core import plot as mp\n",
    "from magpy.core import flagging\n",
    "#%matplotlib widget\n",
    "\n",
    "\n",
    "# Lets load some data from a suspended magnetometer. Such data might be affected by nearby earthquakes\n",
    "\n",
    "yesterday = datetime.now() - timedelta(1)\n",
    "today = datetime.now()\n",
    "data = read(\"https://cobs.zamg.ac.at/gsa/webservice/query.php?id=WIC&starttime={}&endtime={}\".format(yesterday.date(),today.date()))\n",
    "print (\"Got {} data points for data set with sensorid {}\".format(len(data),data.header.get('SensorID')))\n",
    "\n",
    "# Get some information on earthquakes\n",
    "\n",
    "quake = read('https://earthquake.usgs.gov/earthquakes/feed/v1.0/summary/4.5_week.csv')\n",
    "\n",
    "# Here you might want to filter the earthquake data and select only nearby shakes - described elsewhere\n",
    "\n",
    "# Trim the earthquake information to the same timerange as the data set\n",
    "\n",
    "quake = quake.trim(starttime=\"{}\".format(yesterday.date()),endtime=\"{}\".format(today.date()))\n",
    "print (\"Found {} earthquakes\".format(len(quake)))\n",
    "\n",
    "# Transform quake data to a flagging object. Please note that we assign this flagging information to a group of instruments\n",
    "# The group contains xyz keys of the data set imported above.\n",
    "\n",
    "fl = flagging.convert_to_flags(quake, flagtype=2, labelid='030', commentkeys=['M','f',' - ','str3'], groups={data.header.get('SensorID'):['x','y','z']})\n",
    "\n",
    "# Lets get a formated output of the flagging contents\n",
    "\n",
    "fl.fprint(sensorid=quake.header.get('SensorID'))\n",
    "\n",
    "# Then create some patches for the variometer data \n",
    "\n",
    "p = fl.create_patch(data)\n",
    "\n",
    "#Show flagged data in a plot:\n",
    "\n",
    "fig, ax = mp.tsplot(data, ['x'], patch=p, height=2)\n",
    "#fig.savefig(\"/home/leon/Downloads/fl_quakes.png\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9c1e66a0-04a0-4c6d-9a6d-cd7af7760be9",
   "metadata": {},
   "outputs": [],
   "source": [
    "# 6.6. Saving and loading flagging information\n",
    "# ----------------------\n",
    "\n",
    "import sys\n",
    "sys.path.insert(1,'/home/leon/Software/magpy/') # should be magpy2\n",
    "from magpy.stream import *\n",
    "from magpy.core import flagging\n",
    "\n",
    "# Lets create a small flagging object\n",
    "fl = flagging.Flags()\n",
    "fl = fl.add(sensorid=\"LEMI025_X56878_0002_0001\", starttime=\"2022-11-22T16:36:12.654362\",\n",
    "                    endtime=\"2022-11-22T16:41:12.654362\", components=['x', 'y', 'z'], labelid='020', flagtype=4,\n",
    "                    comment=\"SSC with an amplitude of 40 nT\", operator='John Doe')\n",
    "fl = fl.add(sensorid=\"GSM90_Y1112_0001\", starttime=\"2022-11-22T10:56:12.654362\",\n",
    "                          endtime=\"2022-11-22T10:59:12.654362\", components=['f'], flagtype=3, labelid='050')\n",
    "\n",
    "#save this object\n",
    "\n",
    "fl.save(\"/tmp/myflags.json\")\n",
    "\n",
    "# In order to load flagging data use the `load` method of the flagging package\n",
    "\n",
    "fo = flagging.load(\"/tmp/myflags.json\")\n",
    "\n",
    "# The load method can also be used for old flagging data - the debug option might provide some info if anything is not working\n",
    "\n",
    "#fold = flagging.load(\"/tmp/myoldflags.json\", debug=True)\n",
    "\n",
    "# Saving data with flags together\n",
    "\n",
    "data = read(example1)\n",
    "fl = flagging.flag_outlier(data, timerange=120, threshold=3, markall=True)\n",
    "\n",
    "#Now we need to apply the flagging object to the data and insert flagging information directly into the data stream\n",
    "\n",
    "datawithflags = fl.apply_flags(data, mode='insert')\n",
    "\n",
    "#And then we can save this data set with flagging data included as CDF.\n",
    "\n",
    "datawithflags.write('/tmp/',filenamebegins='MyFlaggedExample_', format_type='PYCDF')\n",
    "\n",
    "#To check for a correct saving procedure, read and extract flagging info:\n",
    "\n",
    "fldata = read(\"/tmp/MyFlaggedExample_*\")\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7331b7ce-30d0-4838-86cd-f67a6e27ca84",
   "metadata": {},
   "outputs": [],
   "source": [
    "# 6.7. Advanced flagging operations\n",
    "# ----------------------\n",
    "\n",
    "import sys\n",
    "sys.path.insert(1,'/home/leon/Software/magpy/') # should be magpy2\n",
    "from magpy.stream import *\n",
    "from magpy.core import plot as mp\n",
    "from magpy.core import flagging\n",
    "%matplotlib widget\n",
    "\n",
    "data = read(example1)\n",
    "data = data._drop_column('f')\n",
    "data = data._drop_nans('x')\n",
    "data = data._drop_nans('y')\n",
    "data = data._drop_nans('z')\n",
    "\n",
    "p = mp.tsplot(data,['x','y','z'])\n",
    "\n",
    "# Not yet working - Check\n",
    "\n",
    "fl = flagging.flag_ultra(data)\n",
    "print (fl)\n",
    "\n",
    "#cleandata = fl.apply_flags(data, mode='drop')\n",
    "\n",
    "#mp.tsplot([data,cleandata],[['x','y','z']], symbolcolors=['r','grey'])\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "bbe09b9f-4ce8-4ecf-b152-c5951be5f604",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b1dc4983-5044-4b8f-8a27-b9aa1aa4baea",
   "metadata": {},
   "outputs": [],
   "source": [
    "# ---------------------------------------\n",
    "# Chapter 7: GDI-flux measurements, basevalues and baselines\n",
    "# ---------------------------------------"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2ee9b929-1cd6-4df4-a89a-1da896565e2d",
   "metadata": {},
   "outputs": [],
   "source": [
    "# 7.1. Reading and analyzing DI data\n",
    "# ----------------------\n",
    "\n",
    "from magpy.stream import example6a, example5, DataStream\n",
    "import magpy.absolutes as di\n",
    "from magpy.core.methods import data_for_di\n",
    "import magpy.core.plot as mp\n",
    "\n",
    "absst = di.abs_read(example6a)  # should be the default\n",
    "\n",
    "# You might want to view the data in a formated way\n",
    "\n",
    "for ab in absst:\n",
    "    l1 = ab.get_data_list()\n",
    "    print (l1)\n",
    "    absdata = ab.get_abs_distruct()\n",
    "\n",
    "# DI is used to calculate basevalues for specific instruments. Let us assume you have variometer data from an HEZ \n",
    "# oriented system (this is the expected default). You can just load this data using the magpy stream standard read \n",
    "# method. \n",
    "\n",
    "variodata = read(example5)\n",
    "\n",
    "# You might want to drop flagged data and perform some conversions. Please check the appropriate sections for data manipulation if necessary. \n",
    "# If you do not know (actually you should) if the loaded variometer data (variodata) covers the timerange of the DI measurement (absdata)\n",
    "# then you can use the helper method. \n",
    "\n",
    "vario_rangetest = absdata._check_coverage(variodata,keys=['x','y','z'])\n",
    "\n",
    "# Please note: the variometers H E and Z data is stored at the data keys x, y, z. Do not mix up data keys\n",
    "# used for naming predefined columns and data values asociated with these keys.\n",
    "# As variometer data might not contain individual measurements exactly at the same time as DI measurements were performed\n",
    "# i.e. in case of one-minute variometer data, the variometer data is linearly interpolated and variation data\n",
    "# at times of DI  measurements are extracted at the DI timesteps. Please note: if variation data contains exactly the\n",
    "# timesteps of DI data then exactly the truely measured variation signals are used as linear interpolation only affects\n",
    "# time ranges inbetween variometer data points.\n",
    "# Variation data is then inserted into the absdata structure\n",
    "\n",
    "if vario_rangetest:\n",
    "    func = variodata.interpol(['x','y','z'])\n",
    "    absdata = absdata._insert_function_values(func)\n",
    "\n",
    "# The same procedure can also be performed for independently measured scalar (F) data. \n",
    "\n",
    "scalardata = read(example5)\n",
    "scalar_rangetest = absdata._check_coverage(variodata,keys=['f'])\n",
    "if scalar_rangetest:\n",
    "    func = scalardata.interpol(['f'])\n",
    "    absdata = absdata._insert_function_values(func)\n",
    "\n",
    "# In case your F data is contained in exactly the \n",
    "# same file as variometer data, as it is the case for our example you just might add the 'f' key to interpolation and coverage test\n",
    "# `func = variodata.interpol(['x','y','z','f'])`\n",
    "\n",
    "# After reading DI data and asociating continuous measurements to its time steps it is now time to determine the \n",
    "# absolute values of D and I, and eventually F if not already measured at the main DI pier. DI analysis makes use of a \n",
    "# stepwise algorythm based on an excel sheet by J. Matzka and DTU Copenhagen (see section 7.7 for background information). \n",
    "\n",
    "# This stepwise procedure is automatically performed by the the `calcabsolutes`method, which will iteratively call the \n",
    "# submethods _calddec and calcinc\n",
    "\n",
    "result = absdata.calcabsolutes(usestep=0, annualmeans=None, printresults=True, debugmode=False, \n",
    "                              deltaD=0.0, deltaI=0.0, meantime=False, scalevalue=None, \n",
    "                              variometerorientation='hez', residualsign=1)\n",
    "\n",
    "# What calcabs is auctually doing:\n",
    "\n",
    "# As noted above: calcabsolutes will iteratively call the submethods _calddec and calcinc and gradually improve determinations \n",
    "# of D, I and basevalues. You just need to provide some starting values and then call the following methods. Results from the \n",
    "# previous step are fed into the next step. Here we are using three steps and you can see that the results already stabilzes \n",
    "# after the second step.\n",
    "\n",
    "xyzo = False\n",
    "hstart = 0.0\n",
    "hbasis = 0.0\n",
    "ybasis = 0.0\n",
    "\n",
    "#Step 0\n",
    "resultline, decmeanx, decmeany, variocorrold = absdata._calcdec(xstart=20000,ystart=1700,hstart=hstart,hbasis=hbasis,ybasis=0.0,deltaD=0.0,usestep=0,scalevalue=None,iterator=0,annualmeans=None,meantime=False,xyzorient=xyzo,residualsign=1,debugmode=False)\n",
    "outline, hstart, hbasis = absdata._calcinc(resultline,scalevalue=None,incstart=0.0,deltaI=0.0,iterator=0,usestep=0,annualmeans=None,xyzorient=xyzo,decmeanx=decmeanx,decmeany=decmeany,variocorrold=variocorrold,residualsign=1,debugmode=False)\n",
    "if xyzo:\n",
    "    ybasis = outline.dy\n",
    "print (\"Step0:\", outline[1],outline[2])\n",
    "#Step 1\n",
    "resultline, decmeanx, decmeany, variocorrold = absdata._calcdec(xstart=20000,ystart=1700,hstart=hstart,hbasis=hbasis,ybasis=ybasis,deltaD=0.0,usestep=0,scalevalue=None,iterator=1,annualmeans=None,meantime=False,xyzorient=xyzo,residualsign=1,debugmode=False)\n",
    "inc = outline.x\n",
    "outline, hstart, hbasis = absdata._calcinc(resultline,scalevalue=None,incstart=inc,deltaI=0.0,iterator=1,usestep=0,annualmeans=None,xyzorient=xyzo,decmeanx=decmeanx,decmeany=decmeany,variocorrold=variocorrold,residualsign=1,debugmode=False)\n",
    "if xyzo:\n",
    "    ybasis = outline.dy\n",
    "print (\"Step1:\",outline[1],outline[2])\n",
    "#Step 2\n",
    "resultline, decmeanx, decmeany, variocorrold = absdata._calcdec(xstart=20000,ystart=1700,hstart=hstart,hbasis=hbasis,ybasis=ybasis,deltaD=0.0,usestep=0,scalevalue=None,iterator=2,annualmeans=None,meantime=False,xyzorient=xyzo,residualsign=1,debugmode=False)\n",
    "inc = outline.x\n",
    "outline, hstart, hbasis = absdata._calcinc(resultline,scalevalue=None,incstart=inc,deltaI=0.0,iterator=2,usestep=0,annualmeans=None,xyzorient=xyzo,decmeanx=decmeanx,decmeany=decmeany,variocorrold=variocorrold,residualsign=1,debugmode=False)\n",
    "if xyzo:\n",
    "    ybasis = outline.dy\n",
    "print (\"Step2:\",outline[1],outline[2])\n",
    "\n",
    "print('Declination: %s, Inclination: %s, H: %.1f, Z: %.1f, F: %.1f' % (di.deg2degminsec(outline.y),di.deg2degminsec(outline.x),outline.f*np.cos(outline.x*np.pi/180),outline.f*np.sin(outline.x*np.pi/180),outline.f))\n",
    "print('Collimation and Offset:')\n",
    "print('Declination:    S0: %.3f, delta H: %.3f, epsilon Z: %.3f\\nInclination:    S0: %.3f, epsilon Z: %.3f\\nScalevalue: %.3f deg/unit' % (outline.var1,outline.var2,outline.var3,outline.var4,outline.var5,outline.t2))\n",
    "\n",
    "# deltaF (dF) will be determined if F(abs) are provided along with the DI data. If only contiuous F determinations are available make\n",
    "# sure that this data is already corrected for dF.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2ef96fb7-5e76-40ca-b286-49eb41f6ce5d",
   "metadata": {},
   "outputs": [],
   "source": [
    "# 7.2. The absolute_analysis method - single command DI analysis \n",
    "# ----------------------\n",
    "\n",
    "# Basically everything necessary for DI analysis as shown in 7.1 is available with a single command \n",
    "# Besides all the methods shown above, the absolut analysis command also makes use of two additional\n",
    "# helper methods 'absolutes._analyse_di_source' and 'methods.data_for_di'. The first one allows you\n",
    "# to access DI raw data from various different sources and the second method will perform any selected\n",
    "# correction on contiunuous variometer os scalar data used for DI analysis. More details can found using\n",
    "# help()\n",
    "\n",
    "# Most basic - single files for di data, variometer data and scalar data\n",
    "\n",
    "basevalue = di.absolute_analysis(example6a, example5, example5)\n",
    "\n",
    "# Variometer and Scalar data can be obtained from files, directories, databases and webservices\n",
    "# The same applies for data sources for th DI data. You might even want to define \n",
    "# i.e. both a database source and a file archive. In this case first the database will\n",
    "# be searched for valid data and if not found than the file path will be used\n",
    "\n",
    "# Data sources  - files and databank\n",
    "from magpy.core import database\n",
    "db = database.DataBank(\"localhost\",\"maxmustermann\",\"geheim\",\"testdb\")\n",
    "data = read(example5)\n",
    "db.write(data)\n",
    "basevalues = di.absolute_analysis(example6a, {'file':example5, 'db':(db,'WIC_1_0001_0001')}, example5, db=db, starttime=\"2018-08-28\", endtime=\"2018-08-30\")\n",
    "basevalues = di.absolute_analysis([example6a,example6b], {'file':example5, 'db':(db,'WIC_1_0001_0001')}, example5, db=db, starttime=\"2018-08-28\", endtime=\"2018-08-30\")\n",
    "basevalues = di.absolute_analysis('DIDATA', {'file':example5, 'db':(db,'WIC_1_0001_0001')}, example5, db=db, starttime=\"2018-08-28\", endtime=\"2018-08-30\")\n",
    "\n",
    "# The following options are available to provide DI data:\n",
    "# - database       : 'tablename' ; requires options db, starttime and endtime, recommended option pier (if you use more then one)\n",
    "#                                  i.e. db=database.DataBank(\"localhost\",\"user\",\"pwd\",\"dbname\"), starttime=\n",
    "# - individual file  : \"/path/to/file1.txt\"\n",
    "# - multiple files : [\"/path/to/file1.txt\",\"/path/to/file2.txt\"]\n",
    "# - directory      : \"/directory/with/difiles/\"; requires options starttime and endtime, option diid recommended\n",
    "# - webservice     : TODO ;requires options starttime and endtime,\n",
    "\n",
    "# Depending on the DI data source it migth also be necessary to provide the following options, if this information \n",
    "# is not part of the header (i.e. AutoDIF data, webservices): startionid, pier, azimuth\n",
    "\n",
    "\n",
    "\n",
    "# The following options are available to provide variometer and scalar data:\n",
    "# - database                : {\"db\":(db,\"tablename\")}\n",
    "# - individual file           : \"/path/to/data.cdf\" or {\"file\":\"/path/to/data.cdf\"}\n",
    "# - directory with wildcards : \"/path/with/data/*\" or \"/path/with/data/*.cdf\" or {\"file\":\"/path/to/data/*\"}\n",
    "# - webservice              : \"https://cobs.geosphere.at/gsa/webservice/query.php?id=WIC\" or {\"file\": ...}\n",
    "\n",
    "# In the following a few options are discussed. This is only the tip of the iceberg. If you want to get information about all options\n",
    "# please use help(di.absolute_analysis)\n",
    "\n",
    "# The following options will be shortly discussed: \n",
    "# Basic parameters: variometerorientation (XYZ or HEZ analysis)\n",
    "# Corrections: alpha, beta, deltaF, deltaD, deltaI, compensation, magrotation; \n",
    "# Residual method: residualsign\n",
    "# Thresholds: expD, expI, expT\n",
    "# Archiving successful analysis: movetoarchive and dbadd; TODO code needs to be written and tested (start and endtime seem not to be\n",
    "# considered for filelist), \n",
    "\n",
    "\n",
    "# Archiviung successfully analyzed DI data and database storage\n",
    "\n",
    "basevalues = di.absolute_analysis('DIDATA', {'file':example5, 'db':(db,'WIC_1_0001_0001')}, example5, db=db, starttime=\"2018-08-28\", endtime=\"2018-08-30\", movetoarchive=\"/home/leon/Tmp/\")\n",
    "basevalues = di.absolute_analysis(\"/home/leon/Tmp/2018-08-29_07-42-00_A2_WIC.txt\", {'file':example5, 'db':(db,'WIC_1_0001_0001')}, example5, db=db, starttime=\"2018-08-28\", endtime=\"2018-08-30\", movetoarchive=\"/tmp\")\n",
    "basevalues = di.absolute_analysis(example6b, example5, example5, db=db, dbadd='DIDATA', stationid='WIC')\n",
    "\n",
    "# A typical command as used in the Conrad Observatories automatically scheduled analysis routine looks as follows. Manual data, \n",
    "# as typed in using the xmagpy form, and automatic AutoDIF measurements are collected by a script within an analysis directory. The\n",
    "# successfully analzed data sets are stored in a database and files are moved/stored in an archive - raw -directory. Failed analyses \n",
    "# will remain within the analysis directory for review by the observer.\n",
    "\n",
    "# TODO : F(abs) and F(ext) - examples \n",
    "\n",
    "# "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3689ff08-61ca-420e-9104-25f3be39ad4a",
   "metadata": {},
   "outputs": [],
   "source": [
    "# 7.4 INTERMAGNET IBFV files \n",
    "# ----------------------\n",
    "\n",
    "#see section 3"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9a6a4f1f-e46c-422c-85d9-7906ad50aaa7",
   "metadata": {},
   "outputs": [],
   "source": [
    "# 7.4 Basevalues and baselines\n",
    "# ----------------------\n",
    "\n",
    "#Basevalues as obtained section 7.2 are stored in a normal data stream object, therefore all analysis methods outlined\n",
    "#above can be applied to this data. The `diresult` object contains D, I, and F values for each measurement in columns\n",
    "#x,y,z. Basevalues for H, D and Z related to the selected variometer are stored in columns dx,dy,dz. In `example3`, you\n",
    "#will find some example DI analysis results. To plot these basevalues we can use the following plot command, where we\n",
    "#specify the columns, filled circles as plotsymbols and also define a minimum spread of each y-axis of +/- 2 nT for H \n",
    "#and Z, +/- 0.02 deg for D.\n",
    "\n",
    "basevalues = read(example3)\n",
    "mp.tsplot(basevalues, keys=['dx','dy','dz'], symbols=[['o','o','o']], padding=[[2,0.02,2]])\n",
    "\n",
    "#Fitting a baseline can be easily accomplished with the `fit` method. First we test a linear fit to the data by fitting \n",
    "#a polynomial function with degree 1. We will apply that fir for all data before 2018-05-17\n",
    "\n",
    "func1 = basevalues.fit(['dx','dy','dz'],fitfunc='poly', fitdegree=1, endtime=\"2018-05-17\")\n",
    "mp.tsplot([basevalues], keys=[['dx','dy','dz']], symbols=[['o','o','o']], padding=[[2,0.02,2]], functions=[[func1,func1,func1]])\n",
    "\n",
    "#We then fit a spline function using 3 knotsteps over the remaining timerange (the knotstep option is always related \n",
    "#to the given timerange normalized to 1).\n",
    "\n",
    "func2 = basevalues.fit(['dx','dy','dz'],fitfunc='spline', knotstep=0.33, starttime=\"2018-05-16\")\n",
    "func = [func1,func2]\n",
    "mp.tsplot([basevalues], keys=[['dx','dy','dz']], symbols=[['o','o','o']], padding=[[2,0.02,2]], functions=[[func,func,func]])\n",
    "\n",
    "#Any functional parameters can be added to the meta information of the data set, which can either holf a function or list\n",
    "#of functions\n",
    "\n",
    "basevalues.header['DataFunctionObject'] = func\n",
    "\n",
    "#Hint: a good estimate on the necessary fit complexity can be obtained by looking at delta F values. If delta F is mostly \n",
    "#constant, then the baseline should also not be very complex."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d07c14a6-afa1-4524-a46b-5134cf43651a",
   "metadata": {},
   "outputs": [],
   "source": [
    "# 7.5 Applying baselines\n",
    "# ----------------------\n",
    "\n",
    "#The baseline method provides a number of options to assist the observer in determining baseline corrections and realted issues. \n",
    "#The basic building block of the baseline method is the fit function as discussed above. Lets first load raw vectorial \n",
    "#geomagnetic data, the absevalues of which are contained in above example:\n",
    "\n",
    "rawdata = read(example5)\n",
    "\n",
    "#Now we can apply the basevalue information and the spline function as tested above:\n",
    "\n",
    "func = rawdata.baseline(basevalues, extradays=0, fitfunc='spline',\n",
    "                                knotstep=0.33,startabs='2018-01-01',endabs='2019-01-01')\n",
    "\n",
    "#The `baseline` method will determine and return a fit function between the two given timeranges based on the provided \n",
    "#basevalue data `blvdata`. The option `extradays` allows for adding days before and after start/endtime for which the \n",
    "#baseline function will be extrapolated. This option is useful for providing quasi-definitive data. When applying \n",
    "#this method, a number of new meta-information attributes will be added, containing basevalues and all functional \n",
    "#parameters to describe the baseline. Thus, the stream object still contains uncorrected raw data, but all baseline \n",
    "#correction information is now contained within its meta data. To apply baseline correction you can use the `bc` method:\n",
    "\n",
    "corrdata = rawdata.bc()\n",
    "\n",
    "#Please note that MagPy by defaults expects basevalues for HDZ (see example3.txt). When applying these basevalues \n",
    "#the D-base value is automatically converted to nT and applied to your variation data. Alternatively you can also \n",
    "#use MaPy basevalue files with XYZ basevalues. In order to apply such data correctly, the column names need to contain\n",
    "#the correct names, i.e. X-base, Y-base, Z-base instead of H-base, D-base and Z-base (as in example3.txt).\n",
    "\n",
    "#If baseline jumps/breaks are necessary due to missing data, you can call the baseline function for each independent \n",
    "#segment and combine the resulting baseline functions to  a list. Please note that if no measured data is available at\n",
    "#the time of the baselinejump then extrapolation based on duplication is used or calculating the baseline fit in that\n",
    "#segment.\n",
    "\n",
    "data = read(example5)\n",
    "basevalues = read(example3)\n",
    "adoptedbasefunc = []\n",
    "adoptedbasefunc.append(data.baseline(basevalues, extradays=0, fitfunc='poly', fitdegree=1,startabs='2018-01-01',endabs='2018-05-30'))\n",
    "adoptedbasefunc.append(data.baseline(basevalues, extradays=0, fitfunc='spline', knotstep=0.33,startabs='2018-05-30',endabs='2019-01-01'))\n",
    "\n",
    "corr = data.bc()\n",
    "mp.tsplot(corr)\n",
    "\n",
    "#The combined baseline can be plotted accordingly. Extend the function parameters with each additional segment.\n",
    "\n",
    "mp.tsplot([basevalues], keys=[['dx','dy','dz']], symbols=[['o','o','o']], padding=[[5,0.05,5]], functions=[[adoptedbasefunc,adoptedbasefunc,adoptedbasefunc]])\n",
    "\n",
    "#Adding a baseline for scalar data, which is determined from the delta F values provided within the basevalue data stream:\n",
    "\n",
    "scalarbasefunc = basevalues.baseline(basevalues, keys=['df'], extradays=0, fitfunc='poly', fitdegree=1, startabs='2018-01-01', endabs='2019-01-01')\n",
    "mp.tsplot([basevalues], keys=[['dx','dy','dz','df']], symbols=[['o','o','o','o']], padding=[[5,0.05,5,5]], functions=[[adoptedbasefunc,adoptedbasefunc,adoptedbasefunc,scalarbasefunc]])\n",
    "\n",
    "\n",
    "#Getting dailymeans and correction for scalar baseline can be acomplished by:\n",
    "\n",
    "meandata = data.dailymeans()\n",
    "meandata = meandata.func2stream(scalarbasefunc,mode='sub',keys=['f'],fkeys=['df'])\n",
    "meandata = meandata.delta_f()\n",
    "\n",
    "#Please note that here the function originally determined from the deltaF (df) values of the basevalue data needs to be applied to the F column (f) from the data stream. Before saving we will also extract the baseline parameters from the meta information, which is automatically generated by the `baseline` method.\n",
    "\n",
    "absinfo = data.header.get('DataAbsInfo','')\n",
    "fabsinfo = basevalues.header.get('DataAbsInfo','')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7aed23f1-3137-4a78-a4ed-09fc23cc74bc",
   "metadata": {},
   "outputs": [],
   "source": [
    "# 7.6 Saving basevalue and baseline information\n",
    "# ----------------------\n",
    "\n",
    "#The following will create a BLV file:\n",
    "\n",
    "basevalues.write('/tmp/', coverage='all', format_type='BLV', diff=meandata, year='2018', absinfo=absinfo, deltaF=fabsinfo)\n",
    "\n",
    "#Information on the adopted baselines will be extracted from option `absinfo`. If several functions are provided, \n",
    "#baseline jumps will be automatically inserted into the BLV data file. The output of adopted scalar baselines is \n",
    "#configured by option `deltaF`. If a number is provided, this value is assumed to represent the adopted scalar baseline. \n",
    "#If either 'mean' or 'median' are given (e.g. `deltaF='mean'`), then the mean/median value of all delta F values in \n",
    "#the `basevalues` stream is used, requiring that such data is contained. Providing functional parameters as stored in \n",
    "#a `DataAbsInfo` meta information field, as shown above, will calculate and use the scalar baseline function. \n",
    "#The `meanstream` stream contains daily averages of delta F values between variometer and F measurements and the baseline \n",
    "#adoption data in the meta-information. You can, however, provide all this information manually as well. The typical \n",
    "#way to obtain such a `meanstream` is sketched above."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "fa4a12de-4212-437e-8e28-f5d705c42adc",
   "metadata": {},
   "outputs": [],
   "source": [
    "# ---------------------------------------\n",
    "# Chapter 8: Geomagnetic activity\n",
    "# ---------------------------------------"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d1a8dd3d-ee6f-4a8a-85df-411df20206ef",
   "metadata": {},
   "outputs": [],
   "source": [
    "# 8.1 determination of K indices\n",
    "# ----------------------\n",
    "\n",
    "from magpy.core import activity as act\n",
    "## 8.1 determination of K indices\n",
    "data = read(example4)\n",
    "data2 = data.filter()\n",
    "kvals = act.K_fmi(data2, K9_limit=500, longitude=15.0)\n",
    "print (len(kvals))\n",
    "\n",
    "# use help(act.K_fmi) for more options\n",
    "#help(act.K_fmi)\n",
    "\n",
    "# plotting such data\n",
    "p,ax = mp.tsplot([data2,kvals],keys=[['x','y'],['var1']], ylabelposition=-0.08, symbols=[[\"-\",\"-\"],[\"k\"]], title=\"K value plot\", colors=[[[0.5, 0.5, 0.5],[0.5, 0.5, 0.5]],['r']], grid=True, height=2)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "06e92379-73da-4e42-92f2-33d1a2bf9f15",
   "metadata": {},
   "outputs": [],
   "source": [
    "# 8.2 Geomagnetic storm detection\n",
    "# ----------------------\n",
    "\n",
    "# some methods use the pywavelet package - conda install pywavelets\n",
    "# read magnetic raw data, apply compensation fields to get a reasonable vector, apply rotation to get vector into a reasonable \n",
    "# good geomagnetic reference frame (x will correspond to H) and finally smooth by a hanning window of 15 sec length\n",
    "magdata = read(example4, starttime='2024-05-10', endtime='2024-05-11')\n",
    "magdata = magdata.compensation()\n",
    "alpha = magdata.extract_headerlist('DataRotationAlpha')\n",
    "magdata = magdata.rotation(alpha=alpha)\n",
    "magdata = magdata.smooth('x',window_len=15)\n",
    "# read solar wind parameters from ACE \n",
    "sat_1m = read('/home/leon/GeoSphereCloud/Daten/CobsDaten/ExternalFeatures/Ace/20240510_ace_swepam_1m.txt')\n",
    "sat_5m = read('/home/leon/GeoSphereCloud/Daten/CobsDaten/ExternalFeatures/Ace/20240510_ace_epam_5m.txt')\n",
    "# run detection\n",
    "detection, ssc_list = act.seek_storm(magdata, satdata_1m=sat_1m, satdata_5m=sat_5m, verbose=False, returnsat=True, method='AIC')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f20e1e49-b1a5-4778-8f39-d64fc59e2cf2",
   "metadata": {},
   "outputs": [],
   "source": [
    "#Display with patches created from the flagging dictionary\n",
    "fl = flagging.Flags(ssc_list)\n",
    "rf = fl.select(parameter='sensorid', values=['LEMI'])\n",
    "p1 = rf.create_patch()\n",
    "rf = fl.select(parameter='sensorid', values=['ACE'])\n",
    "p2 = rf.create_patch()\n",
    "mp.tsplot([magdata,sat_1m],[['x','y'],['var2']],patch=[[p1,p1],[p2]])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0677fd97-d408-4921-b143-256144ea7c12",
   "metadata": {},
   "outputs": [],
   "source": [
    "## 8.3 Sq variation\n",
    "# requires emd package\n",
    "import emd\n",
    "teststream = read(\"/home/leon/GeoSphereCloud/Daten/CobsDaten/AllMinute/wic_2022_0000_PT1M_4.cdf\", starttime='2022-03-01',endtime='2022-04-01')\n",
    "sqstream = act.sqbase(teststream, components=['x','y'], baseline_type='joint')\n",
    "p = mp.tsplot([teststream,sqstream],keys=[['x','y']],colors=[[[0.5, 0.5, 0.5],[0.5, 0.5, 0.5]],[[0.58, 0.8, 0.8],[0.58, 0.8, 0.8]]],legend={'legendtext':('data','sq'), 'plotnumber':1})\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2832433d-dd33-41f5-98e4-a1feb6aed87e",
   "metadata": {},
   "outputs": [],
   "source": [
    "# ---------------------------------------\n",
    "# Chapter 9: Databases\n",
    "# ---------------------------------------"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2c6aa3a2-f919-44c0-b37b-194711534d39",
   "metadata": {},
   "outputs": [],
   "source": [
    "# 9.1 Basic commands\n",
    "# ----------------------\n",
    "\n",
    "# run mysql \n",
    "# CREATE DATABASE mymagpydb;\n",
    "# Assign a user and grant privileges\n",
    "\n",
    "# Here we will create some example data set first\n",
    "def create_teststream(startdate=datetime(2022, 11, 21), coverage=86400):\n",
    "        # Create a random data signal with some nan values in x and z\n",
    "        c = 1000  # 1000 nan values are filled at random places\n",
    "        l = coverage\n",
    "        array = [[] for el in DataStream().KEYLIST]\n",
    "        import scipy\n",
    "        win = scipy.signal.windows.hann(60)\n",
    "        a = np.random.uniform(20950, 21000, size=int((l + 2880) / 2))\n",
    "        b = np.random.uniform(20950, 21050, size=int((l + 2880) / 2))\n",
    "        x = scipy.signal.convolve(np.concatenate([a, b], axis=0), win, mode='same') / sum(win)\n",
    "        array[1] = np.asarray(x[1440:-1440])\n",
    "        a = np.random.uniform(1950, 2000, size=int((l + 2880) / 2))\n",
    "        b = np.random.uniform(1900, 2050, size=int((l + 2880) / 2))\n",
    "        y = scipy.signal.convolve(np.concatenate([a, b], axis=0), win, mode='same') / sum(win)\n",
    "        y.ravel()[np.random.choice(y.size, c, replace=False)] = np.nan\n",
    "        array[2] = np.asarray(y[1440:-1440])\n",
    "        a = np.random.uniform(44300, 44400, size=(l + 2880))\n",
    "        z = scipy.signal.convolve(a, win, mode='same') / sum(win)\n",
    "        array[3] = np.asarray(z[1440:-1440])\n",
    "        array[4] = np.asarray(np.sqrt((x * x) + (y * y) + (z * z))[1440:-1440])\n",
    "        var1 = [0] * l\n",
    "        var1[43200:50400] = [1] * 7200\n",
    "        varind = DataStream().KEYLIST.index('var1')\n",
    "        array[varind] = np.asarray(var1)\n",
    "        array[0] = np.asarray([startdate + timedelta(seconds=i) for i in range(0, l)])\n",
    "        teststream = DataStream(header={'SensorID': 'Test_0001_0001'}, ndarray=np.asarray(array, dtype=object))\n",
    "        teststream.header['col-x'] = 'X'\n",
    "        teststream.header['col-y'] = 'Y'\n",
    "        teststream.header['col-z'] = 'Z'\n",
    "        teststream.header['col-f'] = 'F'\n",
    "        teststream.header['unit-col-x'] = 'nT'\n",
    "        teststream.header['unit-col-y'] = 'nT'\n",
    "        teststream.header['unit-col-z'] = 'nT'\n",
    "        teststream.header['unit-col-f'] = 'nT'\n",
    "        teststream.header['col-var1'] = 'Switch'\n",
    "        teststream.header['StationID'] = 'TST'\n",
    "        return teststream\n",
    "\n",
    "teststream1 = create_teststream(startdate=datetime(2022, 11, 22))\n",
    "teststream2 = create_teststream(startdate=datetime(2022, 11, 23))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "aaf9b176-d2ba-4698-88db-8bc1484c979a",
   "metadata": {},
   "outputs": [],
   "source": [
    "# 9.2 Basic usage of a MagPy database\n",
    "# ----------------------\n",
    "\n",
    "from magpy.core import database\n",
    "\n",
    "#Let us assume you have created a database called \"testdb\" and granted access to a user \"maxmustermann\" with \n",
    "#password \"geheim\" on your computer. Connect to the data base:\n",
    "\n",
    "db = database.DataBank(\"localhost\",\"maxmustermann\",\"geheim\",\"testdb\")\n",
    "\n",
    "#You access the database the first time you need to initialize a new database\n",
    "\n",
    "db.dbinit()\n",
    "\n",
    "#This will set up a predefined table structure to be ready for MagPy interaction \n",
    "#Use the `alter` method to updated the table structure to any future version of MagPy\n",
    "\n",
    "db.alter()\n",
    "\n",
    "#Add some datastream to the database\n",
    "\n",
    "db.write(teststream1)\n",
    "\n",
    "#Add some additional data\n",
    "\n",
    "db.write(teststream2)\n",
    "\n",
    "#Reading data is also very simple. Like for standard streams you can also specify *starttime* and *endtime*: \n",
    "\n",
    "data = db.read('Test_0001_0001_0001')\n",
    "\n",
    "#Get some basic information on the current state of the database\n",
    "\n",
    "db.info('stdout')\n",
    "\n",
    "#Check if sensorid is already existining - if not create it and fill with header info\n",
    "\n",
    "db.sensorinfo('Test_0001_0001', {'SensorName' : 'BestSensorontheGlobe'})\n",
    "\n",
    "#Get some single string from tables\n",
    "\n",
    "stationid = db.get_string('DATAINFO', 'Test_0001_0001', 'StationID')\n",
    "print (stationid)\n",
    "\n",
    "#Get tablename of sensor\n",
    "\n",
    "tablename = db.datainfo(teststream1.header.get('SensorID'), {'DataComment' : 'Add something'}, None, stationid)\n",
    "print (tablename)\n",
    "\n",
    "#Generally check if a specific table/data set is existing \n",
    "\n",
    "if db.tableexists('Test_0001_0001_0001'):\n",
    "    print (\" Yes, this table exists\")\n",
    "\n",
    "#Get some recent data lines from this table\n",
    "\n",
    "data = db.get_lines(tablename, 1000)\n",
    "print (len(data))\n",
    "\n",
    "#Get some single numerical data from tables\n",
    "\n",
    "sr =  db.get_float('DATAINFO', teststream1.header.get('SensorID'), 'DataSamplingRate')\n",
    "print (sr)\n",
    "\n",
    "#Lets put some values into the predefined PIERS table which can be used to store information on all piers of your \n",
    "#observatories\n",
    "\n",
    "pierkeys = ['PierID', 'PierName', 'PierType', 'StationID', 'PierLong', 'PierLat', 'PierAltitude', 'PierCoordinateSystem', 'DeltaDictionary','AzimuthDictionary']\n",
    "piervalues1 = ['P1','Karl-Heinzens-Supersockel', 'DI', 'TST', '-34894,584', '310264,811','1088,1366', 'GK M34, EPSG: 31253', 'P2_(2015_(deltaF_-0.13);2017_(deltaF_-0.03);2016_(deltaF_-0.06);2018_(deltaF_-0.09);2019_(deltaF_-0.18);2020_(deltaF_-0.19);2021_(deltaF_-0.17);2022_(deltaF_-0.07);2023_(deltaF_-0.23))','Z64351_180.1559161807, T360-075M1_359.25277, Z64388_272.49390, Z64390_87.46448']\n",
    "piervalues2 = ['P2','Hans-Rüdigers-Megasockel', 'DI', 'TST', 461348.00, 5481741.00,101, 'EPSG:25832', 'P2_(2017_(deltaF_-0.85);2016_(deltaI_-0.00019;deltaD_-0.00729;deltaF_-1.33);2019_(deltaF_-0.41);2020_(deltaF_-0.50);2021_(deltaF_-0.77);2022_(deltaF_-0.86);2023_(deltaF_-0.92))','Z64388_268.99712, Z64390_91.41396']\n",
    "\n",
    "#We are using the `update` command to insert above defined data into the table PIERS. Please note that the update command\n",
    "#will call an \"INSERT INTO\" mysql command if no *condition* is given. Thus the command will fail in case of already \n",
    "#existing inputs. If pier data is already existing add something like *condition=\"PierID LIKE 'P1'\"*.\n",
    "\n",
    "db.update('PIERS', pierkeys, piervalues1)\n",
    "db.update('PIERS', pierkeys, piervalues2)\n",
    "\n",
    "#The update method can also be used to update basically any information in all other predefined tables\n",
    "\n",
    "db.update('SENSORS', ['SensorGroup'], ['magnetism'], condition='SensorID=\"Test_0001_0001\"')\n",
    "\n",
    "#Using select to  generally extract information from any table based on the mysql select command \n",
    "#db.select(field_of_interest, table, search_criteria\n",
    "\n",
    "magsenslist = db.select('SensorID', 'SENSORS', 'SensorGroup = \"magnetism\"')\n",
    "print (\"Mag sens\", magsenslist)\n",
    "tempsenslist = db.select('SensorID', 'SENSORS','SensorElements LIKE \"%T%\"')\n",
    "print (\"Temp sens\", tempsenslist)\n",
    "lasttime = db.select('time', 'Test_0001_0001_0001', expert=\"ORDER BY time DESC LIMIT 1\")\n",
    "print (\"Last time\", lasttime)\n",
    "\n",
    "#There are a few further methods to extract very specific information out of some tables. You can use `get_pier` to \n",
    "#obtain delta values with respect to other pillars\n",
    "\n",
    "value = db.get_pier('P2','P1','deltaF')\n",
    "\n",
    "#The coordinate method is useful to extract coordinates and convert them into any desired new coordinate-system \n",
    "\n",
    "(long, lat) = db.coordinates('P1')\n",
    "\n",
    "#Lets read again some data:\n",
    "\n",
    "data = db.read('Test_0001_0001_0001') # test with all options sql, starttime, endtime\n",
    "\n",
    "#Please note: MagPy2.0 is approximately 14 times faster then MagPy1.x for such operations.\n",
    "\n",
    "data.header['DataSensorOrientation'] = 'HEZ'\n",
    "\n",
    "#Let us change the header information of the data set and then update only the database's header information by the new data header\n",
    "\n",
    "db.update_datainfo('Test_0001_0001_0001', data.header)\n",
    "\n",
    "#Delete contents \n",
    "\n",
    "db.delete('Test_0001_0001_0001', timerange=1)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "471e40e7-84f3-46e8-9f66-b83d748063db",
   "metadata": {},
   "outputs": [],
   "source": [
    "# 9.3 Flagging and databases\n",
    "# ----------------------\n",
    "\n",
    "#Working with flagging information is also supported by the database class. There are three methods which can be\n",
    "#used to store and read flags from databases. An existing flagging object can be stored in the database using the\n",
    "#following command assuming you flags are contained in variable *fl*\n",
    "\n",
    "fl = flagging.load(\"/tmp/myflags.json\")\n",
    "db.flags_to_db(fl)\n",
    "\n",
    "#Reading flags from the database is done similar. Possible options are starttime, endtime, sensorid, comment, key and\n",
    "#flagtype:\n",
    "\n",
    "flnew = db.flags_from_db()\n",
    "\n",
    "#If you want to delete specific flags from the data base use the `flag_to_delete` command and define a parameter and \n",
    "#associated value. Parameter \"all\" will delete all existing flags in the database\n",
    "\n",
    "db.flags_to_delete(parameter=\"operator\", value=\"RL\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "96e33c24-16c8-4f52-97d1-9a830ec8f8b9",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
